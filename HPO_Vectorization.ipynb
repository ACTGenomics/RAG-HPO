{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code processes a JSON file containing Human Phenotype Ontology (HPO) data to extract information about HPO terms, their relationships, and lineages.\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Extracts HPO term information (ID, label, definition, synonyms) from a node in the JSON data.\n",
    "def extract_info(node):\n",
    "    hpo_info = {}\n",
    "    hp_id_match = re.search(r'(HP_\\d+)', node['id'])\n",
    "    if hp_id_match:\n",
    "        hp_id = hp_id_match.group(1)\n",
    "        info_dict = {'label': '', 'definition': '', 'synonyms': []}\n",
    "        lbl = node.get('lbl', '')\n",
    "        if lbl:\n",
    "            info_dict['label'] = lbl\n",
    "        if 'meta' in node and 'definition' in node['meta']:\n",
    "            definition_val = node['meta']['definition'].get('val', '')\n",
    "            if definition_val:\n",
    "                info_dict['definition'] = definition_val\n",
    "        if 'meta' in node and 'synonyms' in node['meta']:\n",
    "            synonyms = [synonym.get('val', '') for synonym in node['meta']['synonyms'] if synonym.get('val', '')]\n",
    "            if synonyms:\n",
    "                info_dict['synonyms'] = synonyms\n",
    "        if info_dict['label'] or info_dict['definition'] or info_dict['synonyms']:\n",
    "            hpo_info[hp_id] = info_dict\n",
    "    return hpo_info\n",
    "\n",
    "# Reads the JSON file, processes each node to extract HPO information, and returns a dictionary of all HPO terms.\n",
    "def process_json_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    all_hpo_info = {}\n",
    "    for graph in data.get('graphs', []):\n",
    "        for node in graph.get('nodes', []):\n",
    "            hpo_info = extract_info(node)\n",
    "            if hpo_info:\n",
    "                all_hpo_info.update(hpo_info)\n",
    "    return all_hpo_info\n",
    "\n",
    "# Recursively finds all ancestral lineages for a given HPO term.\n",
    "def find_lineages(term, lineage=[]):\n",
    "    current_lineage = lineage + [f'{label_map.get(term, \"No label\")} ({term})']\n",
    "    if term not in parent_map:\n",
    "        return [current_lineage]\n",
    "    lineages = []\n",
    "    for parent in parent_map[term]:\n",
    "        lineages.extend(find_lineages(parent, current_lineage))\n",
    "    return lineages\n",
    "\n",
    "# Load and process the JSON data\n",
    "hpo_data = process_json_file('hp.json')\n",
    "\n",
    "# Reads the JSON file and initializes mappings for node labels and parent-child relationships.\n",
    "with open('hp.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "nodes = data['graphs'][0]['nodes']\n",
    "edges = data['graphs'][0]['edges']\n",
    "label_map = {node['id'].split('/')[-1]: node.get('lbl', 'No label') for node in nodes if 'lbl' in node}\n",
    "parent_map = defaultdict(list)\n",
    "for edge in edges:\n",
    "    sub_id = edge['sub'].split('/')[-1]\n",
    "    obj_id = edge['obj'].split('/')[-1]\n",
    "    parent_map[sub_id].append(obj_id)\n",
    "\n",
    "# Create a dictionary to store all lineages for each term\n",
    "# Stores all lineages for each HPO term, excluding obsolete terms.\n",
    "hpo_lineage = {}\n",
    "for term in label_map.keys():\n",
    "    if 'obsolete' in label_map.get(term, '').lower():\n",
    "        continue\n",
    "    lineages = find_lineages(term)\n",
    "    hpo_lineage[term] = [' -> '.join(lineage) for lineage in lineages]\n",
    "\n",
    "# Initialize dictionaries for tracking relationships\n",
    "immediate_parents = defaultdict(set)\n",
    "immediate_descendants = defaultdict(set)\n",
    "all_descendants = defaultdict(set)\n",
    "\n",
    "# Initializes and updates dictionaries to track immediate and all descendants for each HPO term.\n",
    "for term, lineages in hpo_lineage.items():\n",
    "    for lineage in lineages:\n",
    "        terms = [t.split(' ')[-1].strip('()\\n') for t in lineage.split(' -> ')]\n",
    "        for i, term in enumerate(terms):\n",
    "            if i < len(terms) - 1:\n",
    "                immediate_parents[terms[i]].add(terms[i+1])\n",
    "                immediate_descendants[terms[i+1]].add(terms[i])\n",
    "            if i > 0:\n",
    "                for descendant in terms[:i]:\n",
    "                    all_descendants[terms[i]].add(descendant)\n",
    "\n",
    "# Adds relationship counts (unique parents, immediate descendants, total descendants) to each HPO term.\n",
    "for hpo_id in set(immediate_parents.keys()).union(immediate_descendants.keys()).union(all_descendants.keys()):\n",
    "    if hpo_id not in hpo_data:\n",
    "        hpo_data[hpo_id] = {\"Description\": \"No description available\"}\n",
    "    hpo_data[hpo_id][\"Unique_Parent_Count\"] = len(immediate_parents[hpo_id])\n",
    "    hpo_data[hpo_id][\"Immediate_Descendant_Count\"] = len(immediate_descendants[hpo_id])\n",
    "    hpo_data[hpo_id][\"Total_Descendant_Count\"] = len(all_descendants[hpo_id])\n",
    "\n",
    "# Adds sorted lineage information to each HPO term.\n",
    "lineages_by_term = defaultdict(list)\n",
    "for term, lineages in hpo_lineage.items():\n",
    "    lineages_by_term[term].extend(lineages)\n",
    "\n",
    "for hpo_id, lineages in lineages_by_term.items():\n",
    "    sorted_lineages = sorted(lineages, key=lambda x: len(x.split(' -> ')))\n",
    "    if hpo_id in hpo_data:\n",
    "        hpo_data[hpo_id][\"lineage\"] = sorted_lineages\n",
    "    else:\n",
    "        hpo_data[hpo_id] = {\n",
    "            \"Description\": \"No description available\",\n",
    "            \"lineage\": sorted_lineages\n",
    "        }\n",
    "\n",
    "# Saves the updated HPO data with lineage information to a new JSON file.\n",
    "with open('hpo_data_with_lineage.json', 'w') as file:\n",
    "    json.dump(hpo_data, file, indent=4)\n",
    "print(\"The final HPO data with lineage information has been saved to 'hpo_data_with_lineage.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "# Increase the CSV field size limit to handle large fields\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# Paths to the files\n",
    "JSON_FILE_PATH = 'hpo_data_with_lineage.json'\n",
    "CSV_FILE_PATH = 'HPO_addons.csv'\n",
    "OUTPUT_FILE = 'G2GHPO_metadata_test.npy'\n",
    "MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "CSV_OUTPUT_FILE = 'HP_DB_test.csv'  # Output file for manual inspection\n",
    "\n",
    "# Load the CSV data\n",
    "csv_data = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "# Regular expression pattern to remove parentheses and their contents\n",
    "PATTERN = re.compile(r'\\(.*?\\)')\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(PATTERN, '', text).replace('_', ' ').lower()\n",
    "\n",
    "def process_json_file(json_file_path, csv_data):\n",
    "    \"\"\"Processes the JSON file and integrates additional information from the CSV.\"\"\"\n",
    "    data = []\n",
    "    csv_rows = []  # Rows for the CSV output\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        hpo_data = json.load(file)\n",
    "    for hp_id, details in hpo_data.items():\n",
    "        formatted_hp_id = hp_id.replace('_', ':')\n",
    "        unique_info = set()\n",
    "\n",
    "        # Clean and add label\n",
    "        label = details.get('label')\n",
    "        if label:\n",
    "            unique_info.add(clean_text(label))\n",
    "\n",
    "        # Clean and add synonyms\n",
    "        synonyms = details.get('synonyms', [])\n",
    "        for synonym in synonyms:\n",
    "            unique_info.add(clean_text(synonym))\n",
    "\n",
    "        # Clean and add definition\n",
    "        definition = details.get('definition', '')\n",
    "        if definition:\n",
    "            unique_info.add(clean_text(definition))\n",
    "\n",
    "        # Add CSV info if available\n",
    "        csv_addons = csv_data[csv_data['HP_ID'] == formatted_hp_id]['info'].tolist()\n",
    "        for addon in csv_addons:\n",
    "            unique_info.add(clean_text(addon))\n",
    "\n",
    "        # Include lineage information\n",
    "        lineages = details.get('lineage', [])\n",
    "        for info in unique_info:\n",
    "            data.append((formatted_hp_id, info, ', '.join(lineages)))\n",
    "            csv_rows.append({'HP_ID': formatted_hp_id, 'info': info, 'lineage': ', '.join(lineages)})\n",
    "    return data, csv_rows\n",
    "\n",
    "def save_to_csv(csv_rows, output_file):\n",
    "    \"\"\" Saves processed data to a CSV file for manual inspection.\"\"\"\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['HP_ID', 'info', 'lineage'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_rows)\n",
    "    print(f\"Processed data has been written to {output_file} for inspection.\")\n",
    "\n",
    "def calculate_depth(lineage):\n",
    "    \"\"\" Calculates depth of a term based on the lineage hierarchy. \"\"\"\n",
    "    return lineage.count(\"->\") + 1\n",
    "\n",
    "def extract_organ_system(lineage):\n",
    "    \"\"\" Extracts the organ system from the lineage hierarchy. \"\"\"\n",
    "    parts = lineage.split(\"->\")\n",
    "    return parts[1].strip() if len(parts) > 1 else \"Unknown\"\n",
    "\n",
    "def create_vector_database(data, output_file, model_name):\n",
    "    \"\"\" Embeds structured data and saves the embeddings and metadata.\"\"\"\n",
    "    print(\"Initializing embeddings model...\")\n",
    "    embedding_model = TextEmbedding(model_name=model_name)\n",
    "    if os.path.exists(output_file):\n",
    "        print(\"Loading existing embedded documents...\")\n",
    "        embedded_documents = list(np.load(output_file, allow_pickle=True))\n",
    "    else:\n",
    "        print(\"Starting with new embedded documents list...\")\n",
    "        embedded_documents = []\n",
    "    print(f\"Data prepared with {len(data)} terms to embed.\")\n",
    "    batch_size = 100\n",
    "    total_batches = (len(data) + batch_size - 1) // batch_size\n",
    "    print(\"Starting the embedding process...\")\n",
    "    for i in tqdm(range(0, len(data), batch_size), total=total_batches, desc=\"Embedding Texts\"):\n",
    "        batch_data = data[i:i + batch_size]\n",
    "        for hp_id, cleaned_info, lineage in batch_data:\n",
    "            try:\n",
    "                depth = calculate_depth(lineage)\n",
    "                organ_system = extract_organ_system(lineage)\n",
    "                embedding = np.array(list(embedding_model.embed([cleaned_info]))[0])\n",
    "                metadata = {\n",
    "                    'embedding': embedding,\n",
    "                    'metadata': {'info': cleaned_info, 'hp_id': hp_id},\n",
    "                    'lineage': lineage,\n",
    "                    'organ_system': organ_system,\n",
    "                    'depth_from_root': depth\n",
    "                }\n",
    "                embedded_documents.append(metadata)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to embed text due to {e}\")\n",
    "    np.save(output_file, embedded_documents, allow_pickle=True)\n",
    "    print(f\"All embeddings and metadata are saved in: {output_file}\")\n",
    "\n",
    "def main():   \n",
    "    # Process JSON and integrate CSV data\n",
    "    data, csv_rows = process_json_file(JSON_FILE_PATH, csv_data)\n",
    "\n",
    "    # Save the processed data to a CSV for manual inspection\n",
    "    save_to_csv(csv_rows, CSV_OUTPUT_FILE)\n",
    "    print(f\"The database contains {len(data)} entries ready for embedding.\")\n",
    "    # Embed data and save the vector database\n",
    "    create_vector_database(data, OUTPUT_FILE, MODEL_NAME)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
