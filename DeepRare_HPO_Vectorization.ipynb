{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c710ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2390893/502572616.py:51: UnicodeWarning: unsound encoding, assuming ISO-8859-1 (73% confidence)\n",
      "  _hpo_ontology = pronto.Ontology(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ontology with 19657 terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2390893/502572616.py:116: UnicodeWarning: unsound encoding, assuming ISO-8859-1 (73% confidence)\n",
      "  ont     = pronto.Ontology(Path(obo_path))\n",
      "Building HPO DataFrame: 100%|████████████████████████████████████████████████████████████████████████████████████| 19657/19657 [00:01<00:00, 10873.26term/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built DataFrame with 245952 rows → deeprare_hpo_terms_full.csv\n",
      "Loading SBERT model: FremyCompany/BioLORD-2023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e1bc7b086d479b89c30afef7ae6eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02df454b424c494697db2ad2c7866344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28d941e33d3486e937018b096988cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6160773d2d64a10ac7197485c8b16b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6862d64d342a440c82936b9753392b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6556884148f4d8fb6e762bcdf8d9b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7a9da61df44f77a150339a91728548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bc2ea4c7d9428f9c6026c067f72309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe46be728f94896b25766911ad056b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf640bda4444296b7b21904538060fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e41953ae954f0a88f5b6def983d9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding rows: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 245952/245952 [51:14<00:00, 79.99row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 245952 embeddings → deeprare_hpo_meta.json, deeprare_hpo_embedded.npz\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "HPO Vector DB Builder via Pronto Graph Extraction\n",
    "\n",
    "Features:\n",
    "- Automatic OBO download & refresh (`initialize_hpo_resources`)\n",
    "- Build metadata directly from the HPO OBO using pronto:\n",
    "  • labels, definitions, synonyms, ALT IDs, xrefs\n",
    "  • full lineage (root→term) across all inheritance paths\n",
    "  • organ system(s) (direct child under phenotypic abnormality)\n",
    "- Optional SBERT (BioBERT) or BGE embeddings\n",
    "- Antonym `direction` flags\n",
    "- Float16 quantization & compressed storage (.npz)\n",
    "- Optional limit for test extraction\n",
    "\"\"\"\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pronto      # pip install pronto\n",
    "from fastembed import TextEmbedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ────────── 0. Automatic OBO Download & Load ──────────\n",
    "_hpo_ontology = None\n",
    "\n",
    "def initialize_hpo_resources(\n",
    "    obo_url: str = \"https://purl.obolibrary.org/obo/hp.obo\",\n",
    "    obo_path: str = \"hp.obo\",\n",
    "    refresh_days: int = 14,\n",
    ") -> pronto.Ontology:\n",
    "    \"\"\"\n",
    "    Download or refresh the HPO OBO file if older than `refresh_days`,\n",
    "    then load it via pronto.\n",
    "    \"\"\"\n",
    "    global _hpo_ontology\n",
    "    if _hpo_ontology is None:\n",
    "        obo_file = Path(obo_path)\n",
    "        if not obo_file.exists() or ((time.time() - obo_file.stat().st_mtime) / 86400) > refresh_days:\n",
    "            print(f\"Downloading HPO ontology from {obo_url} …\")\n",
    "            resp = requests.get(obo_url, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            obo_file.write_text(resp.text, encoding=\"utf-8\")\n",
    "        with open(obo_file, 'rb') as f:\n",
    "            _hpo_ontology = pronto.Ontology(f)\n",
    "        print(f\"Loaded ontology with {len(list(_hpo_ontology.terms()))} terms\")\n",
    "    return _hpo_ontology\n",
    "\n",
    "ROOT_ID  = 'HP:0000001'  # “All”\n",
    "PHENO_ID = 'HP:0000118'  # “Phenotypic abnormality”\n",
    "\n",
    "# load & cache ontology once\n",
    "ont = initialize_hpo_resources()\n",
    "\n",
    "# maps HP_ID → [immediate parent IDs]\n",
    "parent_map = {\n",
    "    term.id: [p.id for p in term.superclasses(distance=1)]\n",
    "    for term in ont.terms()\n",
    "}\n",
    "\n",
    "# maps HP_ID → term name\n",
    "label_map = {term.id: term.name for term in ont.terms()}\n",
    "\n",
    "# ────────── 1. Multi-Path Lineage Helper ──────────\n",
    "_lineage_memo = {}\n",
    "\n",
    "def _build_lineage_paths(hp_id, parent_map, seen=None):\n",
    "    \"\"\"\n",
    "    Return all paths from ROOT_ID → ... → hp_id.\n",
    "    Each path is a list of HP_ID strings.\n",
    "    Avoids cycles by tracking `seen`.\n",
    "    \"\"\"\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    if hp_id in seen:              # cycle guard\n",
    "        return []\n",
    "    seen = seen | {hp_id}\n",
    "\n",
    "    # cached?\n",
    "    if hp_id in _lineage_memo:\n",
    "        return _lineage_memo[hp_id]\n",
    "\n",
    "    # base case: we hit the root\n",
    "    if hp_id == ROOT_ID:\n",
    "        paths = [[ROOT_ID]]\n",
    "    else:\n",
    "        parents = parent_map.get(hp_id, [])\n",
    "        if not parents:\n",
    "            # orphan: just attach root + self\n",
    "            paths = [[ROOT_ID, hp_id]]\n",
    "        else:\n",
    "            paths = []\n",
    "            for p in parents:\n",
    "                for ppath in _build_lineage_paths(p, parent_map, seen):\n",
    "                    paths.append(ppath + [hp_id])\n",
    "\n",
    "    _lineage_memo[hp_id] = paths\n",
    "    return paths\n",
    "\n",
    "# ────────── 2. Build DataFrame from OBO ──────────\n",
    "CLEAN_ABNORMALITY = re.compile(r'(?i)^Abnormality of(?: the)?\\s*')\n",
    "\n",
    "def _sort_by_numeric(entries: list[str]) -> list[str]:\n",
    "    def key_fn(e: str):\n",
    "        digs = ''.join(filter(str.isdigit, e))\n",
    "        return int(digs) if digs else float('inf')\n",
    "    return sorted(entries, key=key_fn)\n",
    "\n",
    "def build_hpo_dataframe(obo_path: str = \"hp.obo\", limit: int = None) -> pd.DataFrame:\n",
    "    ont     = pronto.Ontology(Path(obo_path))\n",
    "    records = []\n",
    "    terms   = list(ont.terms())[:limit] if limit else list(ont.terms())\n",
    "\n",
    "    for term in tqdm(terms, desc=\"Building HPO DataFrame\", unit=\"term\"):\n",
    "        hp_id      = term.id\n",
    "        label      = term.name\n",
    "        definition = term.definition or \"\"\n",
    "        synonyms   = [syn.description for syn in term.synonyms]\n",
    "\n",
    "        # ── ALT IDs ──\n",
    "        alt_ids = _sort_by_numeric(list(term.alternate_ids))\n",
    "\n",
    "        # ── XREFS ──\n",
    "        snomedct, umls = [], []\n",
    "        for xr in term.xrefs:\n",
    "            txt = str(xr)\n",
    "            m   = re.search(r\"'(.+?:.+?)'\", txt)\n",
    "            ent = m.group(1) if m else txt\n",
    "            pre, _, _ = ent.partition(':')\n",
    "            if pre.upper() == 'UMLS':\n",
    "                umls.append(ent)\n",
    "            elif pre.upper().startswith('SNOMED'):\n",
    "                snomedct.append(ent)\n",
    "        snomedct = _sort_by_numeric(snomedct)\n",
    "        umls      = _sort_by_numeric(umls)\n",
    "\n",
    "        # ── LINEAGE ──\n",
    "        paths = _build_lineage_paths(hp_id, parent_map) or [[ROOT_ID, hp_id]]\n",
    "        for path_ids in paths:\n",
    "            lineage_str = \" -> \".join(f\"{label_map[i]} ({i})\" for i in path_ids)\n",
    "\n",
    "            # ── ORGAN SYSTEM ──\n",
    "            if PHENO_ID in path_ids:\n",
    "                idx   = path_ids.index(PHENO_ID)\n",
    "                organ = label_map.get(path_ids[idx+1], \"Other\") if idx+1 < len(path_ids) else \"Other\"\n",
    "            else:\n",
    "                organ = \"Other\"\n",
    "            organ_system = CLEAN_ABNORMALITY.sub(\"\", organ).title()\n",
    "\n",
    "            # ── EMIT ROWS ──\n",
    "            for phrase in [label] + synonyms:\n",
    "                if not phrase:\n",
    "                    continue\n",
    "                # **Only title-case** the phrase; do NOT strip anything from it\n",
    "                clean_phrase = phrase.title()\n",
    "\n",
    "                records.append({\n",
    "                    \"hp_id\":        hp_id,\n",
    "                    \"phrase\":       clean_phrase,\n",
    "                    \"organ_system\": organ_system,\n",
    "                    \"lineage\":      lineage_str,\n",
    "                    \"definition\":   definition,\n",
    "                    \"alt_ids\":      \";\".join(alt_ids),\n",
    "                    \"snomedct\":     \";\".join(snomedct),\n",
    "                    \"umls\":         \";\".join(umls),\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        records,\n",
    "        columns=[\n",
    "            \"hp_id\", \"phrase\", \"organ_system\", \"lineage\",\n",
    "            \"definition\", \"alt_ids\", \"snomedct\", \"umls\"\n",
    "        ]\n",
    "    )\n",
    "# ────────── 3. Clean Text for Embedding ──────────\n",
    "PAT = re.compile(r'\\s*\\([^)]*\\)\\s*')\n",
    "\n",
    "def clean_text(txt: str) -> str:\n",
    "    txt = PAT.sub(' ', txt)\n",
    "    txt = re.sub(r'\\s+', ' ', txt).strip().lower()\n",
    "    txt = re.sub(r'[^\\w\\s]+$', '', txt)\n",
    "    return txt\n",
    "\n",
    "# ────────── 4. Embedding Model Selector ──────────\n",
    "def get_embedding_model(\n",
    "    use_sbert: bool = True,\n",
    "    sbert_model: str = 'FremyCompany/BioLORD-2023',\n",
    "\n",
    "    bge_model: str = 'BAAI/bge-small-en-v1.5',\n",
    "):\n",
    "    if use_sbert:\n",
    "        print(f\"Loading SBERT model: {sbert_model}\")\n",
    "        return SentenceTransformer(sbert_model)\n",
    "    print(f\"Loading BGE model: {bge_model}\")\n",
    "    return TextEmbedding(model_name=bge_model)\n",
    "\n",
    "# ────────── 5. Vectorize & Save ──────────\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def vectorize_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    meta_out: str,\n",
    "    vec_out: str,\n",
    "    use_sbert: bool = True\n",
    "):\n",
    "    model = get_embedding_model(use_sbert)\n",
    "\n",
    "    # ——— New: collect constant metadata once per HP term ———\n",
    "    constants: dict[str, dict] = {}\n",
    "\n",
    "    # ——— New: per-embedding metadata (minimal) ———\n",
    "    entries: list[dict] = []\n",
    "    embs: list[np.ndarray] = []\n",
    "\n",
    "    # ——— Compile direction‐detection regexes once per call ———\n",
    "    NEG_PATTERN = re.compile(\n",
    "        r'\\b(?:decreas(?:e|ed|ing)?|loss(?:es)?|hypo[-]?\\w+)\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    POS_PATTERN = re.compile(\n",
    "        r'\\b(?:increas(?:e|ed|ing)?|gain(?:s|ed)?|hyper[-]?\\w+)\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Embedding rows\", unit=\"row\"):\n",
    "        info = clean_text(row.phrase)\n",
    "\n",
    "        # ——— More robust direction detection ———\n",
    "        direction = 0\n",
    "        if NEG_PATTERN.search(info):\n",
    "            direction = -1\n",
    "        elif POS_PATTERN.search(info):\n",
    "            direction = 1\n",
    "\n",
    "        # ——— Embedding call unchanged ———\n",
    "        if use_sbert:\n",
    "            vec = model.encode(info, convert_to_numpy=True)\n",
    "        else:\n",
    "            vec = np.asarray(list(model.embed([info]))[0], dtype=np.float32)\n",
    "\n",
    "        # ——— New: store only minimal per-info metadata ———\n",
    "        entries.append({\n",
    "            'hp_id':    row.hp_id,\n",
    "            'info':     info,\n",
    "            'direction':direction\n",
    "        })\n",
    "\n",
    "        # ——— New: record the constant fields once for each hp_id ———\n",
    "        if row.hp_id not in constants:\n",
    "            constants[row.hp_id] = {\n",
    "                'organ_system': row.organ_system,\n",
    "                'lineage':      row.lineage,\n",
    "                'definition':   row.definition,\n",
    "                'alt_ids':      row.alt_ids,\n",
    "                'snomedct':     row.snomedct,\n",
    "                'umls':         row.umls\n",
    "            }\n",
    "\n",
    "        embs.append(vec.astype(np.float16))\n",
    "\n",
    "    # ——— Save embeddings as before ———\n",
    "    emb_matrix = np.vstack(embs)\n",
    "\n",
    "    # ——— Write out a single JSON with both parts ———\n",
    "    combined = {\n",
    "        'constants': constants,\n",
    "        'entries':   entries\n",
    "    }\n",
    "    with open(meta_out, 'w') as f:\n",
    "        json.dump(combined, f, separators=(',', ':'))\n",
    "\n",
    "    np.savez_compressed(vec_out, emb=emb_matrix)\n",
    "    print(f\"Saved {len(entries)} embeddings → {meta_out}, {vec_out}\")\n",
    "\n",
    "# ────────── 6. Main Entrypoint ──────────\n",
    "def main():\n",
    "    df = build_hpo_dataframe(\"hp.obo\")\n",
    "    df.to_csv(\"deeprare_hpo_terms_full.csv\", index=False)\n",
    "    print(f\"Built DataFrame with {len(df)} rows.\")\n",
    "    vectorize_dataframe(\n",
    "        df,\n",
    "        meta_out='deeprare_hpo_meta.json',\n",
    "        vec_out='deeprare_hpo_embedded.npz',\n",
    "        use_sbert=True\n",
    "    )\n",
    "\n",
    "def main_test(\n",
    "    test_limit: int = None\n",
    "):\n",
    "    df = build_hpo_dataframe(\"hp.obo\", limit=test_limit)\n",
    "    out_csv = \"deeprare_hpo_terms_full.csv\" if test_limit is None else f\"hpo_terms_test_{test_limit}.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Built DataFrame with {len(df)} rows → {out_csv}\")\n",
    "    if test_limit is None:\n",
    "        vectorize_dataframe(\n",
    "            df,\n",
    "            meta_out='deeprare_hpo_meta.json',\n",
    "            vec_out='deeprare_hpo_embedded.npz',\n",
    "            use_sbert=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"Test run: skipping vectorization.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import requests\n",
    "# import random\n",
    "# from pathlib import Path\n",
    "\n",
    "# import pronto\n",
    "\n",
    "# # ——— Your existing initialize_hpo_resources() ———\n",
    "# _hpo_ontology = None\n",
    "# def initialize_hpo_resources(\n",
    "#     obo_url: str = \"https://purl.obolibrary.org/obo/hp.obo\",\n",
    "#     obo_path: str = \"hp.obo\",\n",
    "#     refresh_days: int = 14,\n",
    "# ) -> pronto.Ontology:\n",
    "#     global _hpo_ontology\n",
    "#     if _hpo_ontology is None:\n",
    "#         obo_file = Path(obo_path)\n",
    "#         if (\n",
    "#             not obo_file.exists()\n",
    "#             or ((time.time() - obo_file.stat().st_mtime) / 86400) > refresh_days\n",
    "#         ):\n",
    "#             print(f\"Downloading HPO ontology from {obo_url} …\")\n",
    "#             resp = requests.get(obo_url, timeout=30)\n",
    "#             resp.raise_for_status()\n",
    "#             obo_file.write_text(resp.text, encoding=\"utf-8\")\n",
    "#         with open(obo_file, 'rb') as f:\n",
    "#             _hpo_ontology = pronto.Ontology(f)\n",
    "#         print(f\"Loaded ontology with {len(list(_hpo_ontology.terms()))} terms\")\n",
    "#     return _hpo_ontology\n",
    "\n",
    "# def print_random_term():\n",
    "#     ont = initialize_hpo_resources()\n",
    "#     term = random.choice(list(ont.terms()))\n",
    "\n",
    "#     print(\"\\n=== Random HPO Term ===\")\n",
    "#     print(f\"ID         : {term.id}\")\n",
    "#     print(f\"Name       : {term.name}\")\n",
    "#     print(f\"Definition : {term.definition!r}\")\n",
    "\n",
    "#     # Synonyms\n",
    "#     syns = [syn.description for syn in term.synonyms]\n",
    "#     print(f\"Synonyms   : {syns}\")\n",
    "\n",
    "#     # Xrefs\n",
    "#     xrs = [str(x) for x in term.xrefs]\n",
    "#     print(f\"Xrefs      : {xrs}\")\n",
    "\n",
    "#     # Try alt_ids or other_ids or whatever exists\n",
    "#     for attr in (\"other_ids\", \"alt_ids\", \"ids\", \"synonyms\", \"xrefs\"):\n",
    "#         if hasattr(term, attr):\n",
    "#             val = getattr(term, attr)\n",
    "#             print(f\"{attr!r:12}: {val!r}\")\n",
    "\n",
    "#     # If none of those printed, dump dir()\n",
    "#     known = {\"id\",\"name\",\"definition\",\"synonyms\",\"xrefs\",\"other_ids\",\"alt_ids\",\"ids\"}\n",
    "#     present = set(dir(term))\n",
    "#     if not any(a in present for a in (\"other_ids\",\"alt_ids\",\"ids\")):\n",
    "#         print(\"\\n-- No alt_ids-like attribute found; available attributes: --\")\n",
    "#         print(sorted(present))\n",
    "#     print(\"========================\\n\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print_random_term()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f3688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
