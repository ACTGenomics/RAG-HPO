{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf85ab78-32e0-4a7f-8474-2df9e5096cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization has already been completed. Skipping.\n",
      "2024-12-18 16:24:07 - Starting the HPO term extraction process.\n",
      "2024-12-18 16:24:07 - No temporary files found. Starting a new processing run.\n",
      "2024-12-18 16:24:20 - Processing clinical notes.\n",
      "2024-12-18 16:24:20 - Initializing embeddings model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03019398066c4e57adb95c2176845fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-18 16:24:20 - Loading embedded documents\n",
      "2024-12-18 16:24:21 - Preparing embeddings list\n",
      "2024-12-18 16:24:21 - Creating FAISS index\n",
      "2024-12-18 16:24:21 - Processing 1 new clinical notes.\n",
      "2024-12-18 16:24:21 - Processing clinical note for patient_id 1: A 32-year-old man presented to...\n",
      "TOTAL_TOKENS_USED before query: 1885\n",
      "2024-12-18 16:24:26 - Processing exact and non-exact matches.\n",
      "Exact match found: HP:0001903\n",
      "Exact match found: HP:0001250\n",
      "Exact match found: HP:0012185\n",
      "Exact match found: HP:0003002\n",
      "Exact match found: HP:0005231\n",
      "Exact match found: HP:0001009\n",
      "2024-12-18 16:24:26 - Number of unprocessed non-exact matches: 17\n",
      "2024-12-18 16:24:26 - Generating HPO terms for non-exact matches.\n",
      "2024-12-18 16:24:26 - Processing 17 non-exact matches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 2367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1/17 [00:04<01:09,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 2613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2/17 [00:08<01:04,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 2896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3/17 [00:12<01:00,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 3152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 4/17 [00:17<00:55,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 3437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 5/17 [00:21<00:51,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 3769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 6/17 [00:25<00:47,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 4021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 7/17 [00:30<00:42,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 4331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 8/17 [00:34<00:38,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 4656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 9/17 [00:38<00:34,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 4943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 10/17 [00:42<00:30,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 11/17 [00:47<00:25,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 5619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 12/17 [00:51<00:21,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 5923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 13/17 [00:55<00:17,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 6298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 14/17 [01:00<00:12,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 6559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 15/17 [01:04<00:08,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 6872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 16/17 [01:08<00:04,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_TOKENS_USED before query: 7132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:13<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-18 16:25:39 - Compiling final results.\n",
      "+----+--------+--------------------------------+--------------------+\n",
      "|    | Case   | Phenotype name                 | HPO ID             |\n",
      "|----+--------+--------------------------------+--------------------|\n",
      "|  0 | Case 1 | anaemia                        | HP:0001903         |\n",
      "|  1 | Case 1 | epilepsy                       | HP:0001250         |\n",
      "|  2 | Case 1 | carpal tunnel syndrome         | HP:0012185         |\n",
      "|  3 | Case 1 | breast cancer                  | HP:0003002         |\n",
      "|  4 | Case 1 | chronic gastritis              | HP:0005231         |\n",
      "|  5 | Case 1 | telangiectasia                 | HP:0001009         |\n",
      "|  6 | Case 1 | heart arrhythmia               | HP:0011675         |\n",
      "|  7 | Case 1 | c5 nerve root compression      | HP:0003406         |\n",
      "|  8 | Case 1 | bowel cancer                   | HP:0003003         |\n",
      "|  9 | Case 1 | gastric mucosa                 | HP:0004295         |\n",
      "| 10 | Case 1 | minor foveolar hyperplasia     | HP:0000493         |\n",
      "| 11 | Case 1 | stromal oedema                 | No HPO terms found |\n",
      "| 12 | Case 1 | chronic inflammatory cells     | HP:0033196         |\n",
      "| 13 | Case 1 | helicobacter heilmannii        | HP:0032166         |\n",
      "| 14 | Case 1 | hyperplastic changes           | HP:0025092         |\n",
      "| 15 | Case 1 | fundic gland polyp             | HP:0033769         |\n",
      "| 16 | Case 1 | helicobacter serology          | HP:0005202         |\n",
      "| 17 | Case 1 | ca 72–4 tumour marker          | HP:0031030         |\n",
      "| 18 | Case 1 | juvenile polyp                 | HP:0004784         |\n",
      "| 19 | Case 1 | irregular microvascular        | HP:0025016         |\n",
      "| 20 | Case 1 | irregular microsurface pattern | HP:0003025         |\n",
      "| 21 | Case 1 | kudo’s pit pattern             | HP:0008523         |\n",
      "| 22 | Case 1 | inflammatory juvenile polyps   | HP:0012198         |\n",
      "+----+--------+--------------------------------+--------------------+\n",
      "2024-12-18 17:43:23 - Total execution time: 4756.65 seconds\n"
     ]
    }
   ],
   "source": [
    "from contextlib import contextmanager\n",
    "from datetime import datetime\n",
    "from fastembed import TextEmbedding\n",
    "from fuzzywuzzy import fuzz\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from requests.exceptions import HTTPError\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import ast\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"Class to manage LLM API configurations and queries.\"\"\"\n",
    "    def __init__(self, api_key, base_url, model_name=\"llama3-groq-70b-8192-tool-use-preview\", max_tokens_per_day=500000, max_queries_per_minute=30, temperature=0.7):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model_name = model_name\n",
    "        self.max_tokens_per_day = max_tokens_per_day\n",
    "        self.max_queries_per_minute = max_queries_per_minute\n",
    "        self.total_tokens_used = 0\n",
    "        self.temperature = temperature\n",
    "        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def query(self, user_input, system_message):\n",
    "        \"\"\"Sends a query to the LLM API.\"\"\"\n",
    "        print(f\"TOTAL_TOKENS_USED before query: {self.total_tokens_used}\")\n",
    "\n",
    "        # Check token limit\n",
    "        estimated_tokens = len(user_input.split()) + len(system_message.split())\n",
    "        if self.total_tokens_used + estimated_tokens > self.max_tokens_per_day:\n",
    "            raise Exception(\"Token limit exceeded for the day.\")\n",
    "\n",
    "        # Enforce rate limit\n",
    "        time.sleep(60 / self.max_queries_per_minute)\n",
    "\n",
    "        # Construct the payload\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,  # Use the temperature from the instance\n",
    "        }\n",
    "\n",
    "        # Send the request\n",
    "        response = requests.post(self.base_url, headers=self.headers, json=payload)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "\n",
    "        # Update token usage\n",
    "        self.total_tokens_used += estimated_tokens\n",
    "\n",
    "        # Parse and return the response content\n",
    "        result = response.json()\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"] if \"choices\" in result else \"No content returned.\"\n",
    "\n",
    "# Global Constants\n",
    "FLAG_FILE = \"process_completed.flag\"\n",
    "INITIALIZED = False  # Tracks if the function has been called\n",
    "TOTAL_TOKENS_USED = 0  # Initialize token usage tracking globally\n",
    "MAX_QUERIES_PER_MINUTE = 30\n",
    "MAX_TOKENS_PER_DAY = 500000\n",
    "MAX_QUERIES_PER_DAY = MAX_QUERIES_PER_MINUTE * 60 * 24\n",
    "\n",
    "def load_prompts(file_path=\"system_prompts.json\"):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "# Load prompts\n",
    "prompts = load_prompts()\n",
    "\n",
    "# Access prompts when needed\n",
    "system_message_I = prompts[\"system_message_I\"]\n",
    "system_message_II = prompts[\"system_message_II\"]\n",
    "\n",
    "def initialize_groq_environment():\n",
    "    \"\"\"Initializes the LLMClient with user input or default settings.\"\"\"\n",
    "    global INITIALIZED, llm_client\n",
    "\n",
    "    if INITIALIZED:\n",
    "        print(\"GROQ environment is already initialized. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Gather inputs from the user\n",
    "    api_key = input(\"Enter your API key (required): \").strip()\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key is required to proceed.\")\n",
    "\n",
    "    base_url = input(\"Enter base URL for API use (default 'https://api.groq.com/openai/v1/chat/completions'): \").strip()\n",
    "    base_url = base_url if base_url else \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "\n",
    "    model_name = input(\"Enter LLM model name (default 'llama3-groq-70b-8192-tool-use-preview'): \").strip()\n",
    "    model_name = model_name if model_name else \"llama3-groq-70b-8192-tool-use-preview\"\n",
    "\n",
    "    max_tokens_per_day = input(\"Enter max tokens per day (default 500000): \").strip()\n",
    "    max_tokens_per_day = int(max_tokens_per_day) if max_tokens_per_day else MAX_TOKENS_PER_DAY\n",
    "\n",
    "    max_queries_per_minute = input(\"Enter max queries per minute (default 30): \").strip()\n",
    "    max_queries_per_minute = int(max_queries_per_minute) if max_queries_per_minute else MAX_QUERIES_PER_MINUTE\n",
    "\n",
    "    temperature = input(\"Enter the temperature for the model (default 0.2, range 0.0-1.0): \").strip()\n",
    "    try:\n",
    "        temperature = float(temperature) if temperature else 0.2\n",
    "        if not 0.0 <= temperature <= 1.0:\n",
    "            raise ValueError\n",
    "    except ValueError:\n",
    "        print(\"Invalid temperature value. Setting to default (0.7).\")\n",
    "        temperature = 0.7\n",
    "\n",
    "    # Create the LLMClient instance\n",
    "    llm_client = LLMClient(api_key, base_url, model_name, max_tokens_per_day, max_queries_per_minute)\n",
    "    llm_client.temperature = temperature  # Add the temperature to the client object\n",
    "\n",
    "    # Mark initialization as complete\n",
    "    INITIALIZED = True\n",
    "\n",
    "    # Save process flag\n",
    "    with open(FLAG_FILE, \"w\") as flag:\n",
    "        flag.write(\"Process completed.\")\n",
    "\n",
    "    print(\"GROQ environment successfully initialized.\")\n",
    "    print(f\"  API Key: {'*' * len(api_key)} (hidden)\")\n",
    "    print(f\"  Base URL: {base_url}\") \n",
    "    print(f\"  Max Queries Per Minute: {max_queries_per_minute}\")\n",
    "    print(f\"  Max Tokens Per Day: {max_tokens_per_day}\")\n",
    "    print(f\"  Model Name: {model_name}\")\n",
    "    print(f\"  Temperature: {temperature}\")\n",
    "\n",
    "def generate_hpo_terms(df, system_message):\n",
    "    \"\"\"Generates HPO terms from a DataFrame of phrases and metadata.\"\"\"\n",
    "    responses = []\n",
    "    for _, row in df.iterrows():\n",
    "        user_input = row['phrase']\n",
    "        unique_metadata_list = row['unique_metadata']\n",
    "        original_sentence = row['original_sentence']\n",
    "        \n",
    "        # Prepare the human message with context\n",
    "        context_items = []\n",
    "        for item in unique_metadata_list:\n",
    "            parsed_item = clean_and_parse(item)\n",
    "            if parsed_item:\n",
    "                for description, hp_id in parsed_item.items():\n",
    "                    context_items.append(f\"- {description} ({hp_id})\")\n",
    "\n",
    "        context_text = '\\n'.join(context_items)\n",
    "        human_message = (f\"Query: {user_input}\\n\"\n",
    "                         f\"Original Sentence: {original_sentence}\\n\"\n",
    "                         f\"Context: The following related information is available to assist in determining the appropriate HPO terms:\\n\"\n",
    "                         f\"{context_text}\")\n",
    "        \n",
    "        # Query the LLM using the unified function\n",
    "        response_text = llm_client.query(human_message, system_message)\n",
    "        \n",
    "        # Extract HPO terms with regex\n",
    "        hpo_terms = re.findall(r'HP:\\d+', response_text)\n",
    "        if hpo_terms:\n",
    "            responses.append({\"phrase\": user_input, \"response\": ', '.join(hpo_terms)})\n",
    "        else:\n",
    "            responses.append({\"phrase\": user_input, \"response\": 'No HPO terms found'})\n",
    "    \n",
    "    return pd.DataFrame(responses)\n",
    "\n",
    "# Context manager for subprocess handling\n",
    "@contextmanager\n",
    "def managed_subprocess(*args, **kwargs):\n",
    "    \"\"\"Manages a subprocess, ensuring it terminates properly upon completion or failure.\"\"\"\n",
    "    proc = subprocess.Popen(*args, **kwargs)\n",
    "    try:\n",
    "        yield proc\n",
    "    finally:\n",
    "        proc.terminate()  # Ensures proper termination of the subprocess\n",
    "        proc.wait()\n",
    "\n",
    "# Function for printing with timestamps\n",
    "def timestamped_print(message):\n",
    "    \"\"\"Prints a message with the current timestamp for easy log tracking.\"\"\"\n",
    "    print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - {message}\")\n",
    "\n",
    "# Embeddings related functions\n",
    "def initialize_embeddings_model():\n",
    "    \"\"\"Initializes the embeddings model for processing clinical notes.\"\"\"\n",
    "    model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "    try:\n",
    "        embeddings_model = TextEmbedding(model_name=model_name)\n",
    "        return embeddings_model\n",
    "    except Exception:\n",
    "        exit(1)  # Exit on failure to initialize the embeddings model\n",
    "        print(\"Error: Unable to initialize the embeddings model.\")\n",
    "\n",
    "def load_embedded_documents(file_path):\n",
    "    \"\"\"Loads embedded documents (embeddings) from a given file path.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        return np.load(file_path, allow_pickle=True)\n",
    "    else:\n",
    "        exit(1)  # Exit if file not found\n",
    "        print(\"Error: File not found.\")\n",
    "\n",
    "def prepare_embeddings_list(embedded_documents):\n",
    "    \"\"\"Converts the embedded documents into a NumPy array of embeddings.\"\"\"\n",
    "    embeddings_list = [np.array(doc['embedding']) for doc in embedded_documents if isinstance(doc['embedding'], np.ndarray) and doc['embedding'].size > 0]\n",
    "    if not embeddings_list:\n",
    "        exit(1)  # Exit if no valid embeddings found\n",
    "    first_embedding_size = embeddings_list[0].shape[0]  # Ensure uniform embedding size\n",
    "    return np.vstack([emb for emb in embeddings_list if emb.shape[0] == first_embedding_size])\n",
    "\n",
    "def create_faiss_index(embeddings_array):\n",
    "    \"\"\"Creates a FAISS index for efficient similarity searching on embeddings.\"\"\"\n",
    "    if embeddings_array.dtype != np.float32:\n",
    "        embeddings_array = embeddings_array.astype(np.float32)  # Ensure the correct data type for FAISS\n",
    "    dimension = embeddings_array.shape[1]  # Determine embedding dimensionality\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 (Euclidean) distance index\n",
    "    index.add(embeddings_array)  # Add embeddings to FAISS index\n",
    "    return index\n",
    "\n",
    "def process_row(clinical_note, system_message_I, embeddings_model, index, embedded_documents):\n",
    "    \"\"\"Processes a single clinical note by extracting findings and matching metadata.\"\"\"\n",
    "    findings_text = llm_client.query(clinical_note, system_message_I)  # Extract findings from clinical note\n",
    "    if not findings_text:\n",
    "        return None  # If no findings, skip processing\n",
    "\n",
    "    findings = extract_findings(findings_text)  # Extract findings from the note\n",
    "    if not findings:\n",
    "        return None  # Skip processing if no findings are extracted\n",
    "\n",
    "    results_df = process_findings(findings, clinical_note, embeddings_model, index, embedded_documents)  # Process and match findings to metadata\n",
    "    return results_df\n",
    "\n",
    "def extract_findings(response_content):\n",
    "    \"\"\"Extracts findings (key information) from the response content generated by the LLM.\"\"\"\n",
    "    try:\n",
    "        data = json.loads(response_content)  # Parse the JSON content\n",
    "        findings = data.get(\"findings\", [])\n",
    "        return findings\n",
    "    except json.JSONDecodeError:\n",
    "        return []  # Return an empty list if the content cannot be parsed\n",
    "\n",
    "def process_findings(findings, clinical_note, embeddings_model, index, embedded_documents):\n",
    "    \"\"\"Matches findings with their most relevant metadata entries from embeddings.\"\"\"\n",
    "    results = []\n",
    "    # Split the clinical note into sentences for matching findings to specific contexts\n",
    "    sentences = clinical_note.split('.')\n",
    "    \n",
    "    for finding in findings:\n",
    "        # Embed the query phrase (finding) using the embeddings model\n",
    "        query_vector = np.array(list(embeddings_model.embed([finding]))[0]).astype(np.float32).reshape(1, -1)\n",
    "        distances, indices = index.search(query_vector, 800)  # Search for the most similar embeddings\n",
    "        \n",
    "        seen_metadata = set()\n",
    "        unique_metadata = []\n",
    "\n",
    "        for idx in indices[0]:\n",
    "            # Retrieve metadata for the matched embedding\n",
    "            metadata = embedded_documents[idx]['unique_metadata']\n",
    "            metadata_str = json.dumps(metadata)  # Convert the metadata dict to a string\n",
    "\n",
    "            if metadata_str not in seen_metadata:\n",
    "                seen_metadata.add(metadata_str)  # Track unique metadata\n",
    "                unique_metadata.append(metadata_str)\n",
    "                if len(unique_metadata) == 20:  # Limit to the first 20 unique metadata items\n",
    "                    break\n",
    "\n",
    "        # Find the best matching sentence from the clinical note for the finding\n",
    "        finding_words = set(re.findall(r'\\b\\w+\\b', finding.lower()))\n",
    "        best_match_sentence = None\n",
    "        max_matching_words = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_words = set(re.findall(r'\\b\\w+\\b', sentence.lower()))\n",
    "            common_words = finding_words & sentence_words\n",
    "\n",
    "            if len(common_words) > max_matching_words:\n",
    "                max_matching_words = len(common_words)\n",
    "                best_match_sentence = sentence.strip()  # Store the sentence with the most matching words\n",
    "\n",
    "        # Store the results for this finding\n",
    "        results.append({\n",
    "            \"phrase\": finding,\n",
    "            \"unique_metadata\": unique_metadata,  # Save unique metadata for this finding\n",
    "            \"original_sentence\": best_match_sentence})  # Save the best matching sentence \n",
    "\n",
    "    # Convert the results to a DataFrame and save as a CSV file\n",
    "    # The resulting CSV file will contain the extracted findings and their matched metadata \n",
    "    faiss_results_df = pd.DataFrame(results)\n",
    "    faiss_results_df.to_csv('faiss_search_results.csv', index=False)\n",
    "    return faiss_results_df  # Return the DataFrame of results\n",
    "\n",
    "# Helper functions for text cleaning and metadata processing\n",
    "#Cleans and parses a JSON string by fixing formatting issues.\n",
    "def clean_and_parse(json_str):\n",
    "    try:\n",
    "        json_str = json_str.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
    "        json_str = re.sub(r'\\s+', ' ', json_str)  # Remove excess whitespace\n",
    "        return json.loads(json_str)  # Return parsed JSON\n",
    "    except json.JSONDecodeError:\n",
    "        return None  # Return None if the string cannot be parsed\n",
    "\n",
    "def process_unique_metadata(metadata):\n",
    "    \"\"\"Processes unique metadata by converting all keys to lowercase.\"\"\"\n",
    "    if isinstance(metadata, list):\n",
    "        processed_list = []\n",
    "        for item in metadata:\n",
    "            try:\n",
    "                item_dict = json.loads(item)  # Convert string back to dictionary\n",
    "                processed_item = {k.lower(): v for k, v in item_dict.items()}  # Make keys lowercase\n",
    "                processed_list.append(json.dumps(processed_item))  # Convert back to string\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                continue\n",
    "        return processed_list  # Return the list of processed metadata\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans input text by converting to lowercase and removing punctuation.\"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)).strip()  # Remove punctuation and trim whitespace\n",
    "    return text\n",
    "\n",
    "def extract_hpo_term(phrase, metadata_list):\n",
    "    \"\"\" Extracts HPO terms by matching phrases against a list of metadata. The metadata is loaded from the unique_metadata field in the npy array.\"\"\"\n",
    "    cleaned_phrase = clean_text(phrase)  # Clean the input phrase\n",
    "    fuzzy_matches = []\n",
    "\n",
    "    # Step 1: Fuzzy Matching with Metadata\n",
    "    for metadata in metadata_list:\n",
    "        try:\n",
    "            # Convert metadata string to dictionary if necessary\n",
    "            metadata_dict = json.loads(metadata) if isinstance(metadata, str) else metadata\n",
    "            for term, hp_id in metadata_dict.items():\n",
    "                cleaned_term = clean_text(term)  # Clean the metadata term\n",
    "                if fuzz.ratio(cleaned_phrase, cleaned_term) > 80:  # Check for high similarity\n",
    "                    fuzzy_matches.append({term: hp_id})\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            continue  # Skip invalid metadata entries\n",
    "\n",
    "    # If we have fuzzy matches, extend the list\n",
    "    if fuzzy_matches:\n",
    "        metadata_list.extend([json.dumps(match) for match in fuzzy_matches])\n",
    "\n",
    "    # Step 2: Exact Substring Matching\n",
    "    exact_matches = []\n",
    "    for metadata in metadata_list:\n",
    "        try:\n",
    "            metadata_dict = json.loads(metadata) if isinstance(metadata, str) else metadata\n",
    "            for term, hp_id in metadata_dict.items():\n",
    "                cleaned_term = clean_text(term)  # Clean the metadata term\n",
    "                if cleaned_term in cleaned_phrase:  # Check if term is a substring of the phrase\n",
    "                    exact_matches.append({term: hp_id})\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            continue  # Skip invalid metadata entries\n",
    "\n",
    "    # If we have exact matches, extend the list\n",
    "    if exact_matches:\n",
    "        metadata_list.extend([json.dumps(match) for match in exact_matches])\n",
    "\n",
    "    # Step 3: Exact Matching within Metadata List\n",
    "    for metadata in metadata_list:\n",
    "        if not metadata.strip():\n",
    "            continue\n",
    "        try:\n",
    "            metadata_dict = json.loads(metadata) if isinstance(metadata, str) else metadata\n",
    "            for term, hp_id in metadata_dict.items():\n",
    "                cleaned_term = clean_text(term)\n",
    "                if cleaned_phrase == cleaned_term:  # Check for an exact match\n",
    "                    print(f\"Exact match found: {hp_id}\")\n",
    "                    return hp_id  # Return the exact match\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            continue  # Skip invalid metadata entries\n",
    "\n",
    "    return None  # Return None if no match is found\n",
    "\n",
    "\n",
    "# Process the results\n",
    "def process_results(final_result_df):\n",
    "    save_or_display = input(\"Do you want to save the results as a CSV file or display them? (save/display): \").strip().lower()\n",
    "    \n",
    "    if save_or_display == 'save':\n",
    "        output_file = input(\"Enter the name of the output file (with .csv extension): \").strip()\n",
    "        \n",
    "        # Prepare the structured data for saving\n",
    "        new_data = []\n",
    "        for idx, row in final_result_df.iterrows():\n",
    "            patient_id = row['patient_id']\n",
    "            hpo_terms = row['HPO_Terms']  # Directly access the list of dictionaries\n",
    "            \n",
    "            # Process each term\n",
    "            for term in hpo_terms:\n",
    "                phrase = term.get('phrase', '').strip()\n",
    "                hpo_id = term.get('HPO_Term', '').replace(\"HP:HP:\", \"HP:\")\n",
    "                \n",
    "                if not hpo_id:\n",
    "                    print(f\"Blank HPO_Term for patient_id {patient_id} with phrase '{phrase}'\")\n",
    "                \n",
    "                new_data.append({\n",
    "                    'Patient ID': patient_id,\n",
    "                    'Phenotype name': phrase,\n",
    "                    'HPO ID': hpo_id\n",
    "                })\n",
    "        \n",
    "        # Create a new DataFrame and save it\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        new_df.to_csv(output_file, index=False)\n",
    "        json_csv_file = f\"{output_file}_json.csv\"\n",
    "        final_result_df.to_csv(json_csv_file, index=False)\n",
    "        timestamped_print(f\"Data has been successfully saved to {output_file}\")\n",
    "    \n",
    "    elif save_or_display == 'display':\n",
    "        # Prepare the structured data for display\n",
    "        flattened_data = []\n",
    "        for idx, row in final_result_df.iterrows():\n",
    "            patient_id = row['patient_id']\n",
    "            hpo_terms = row['HPO_Terms']  # Directly access the list of dictionaries\n",
    "            \n",
    "            # Process each term\n",
    "            for term in hpo_terms:\n",
    "                flattened_data.append({\n",
    "                    'Case': f\"Case {patient_id}\",\n",
    "                    'Phenotype name': term.get('phrase', '').strip(),\n",
    "                    'HPO ID': term.get('HPO_Term', '').replace(\"HP:HP:\", \"HP:\")\n",
    "                })\n",
    "        \n",
    "        if flattened_data:\n",
    "            flattened_df = pd.DataFrame(flattened_data)\n",
    "            print(tabulate(flattened_df, headers='keys', tablefmt='psql'))\n",
    "        else:\n",
    "            timestamped_print(\"No HPO terms found to display.\")\n",
    "    else:\n",
    "        print(\"Invalid choice. Please choose either 'save' or 'display'.\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    if check_and_initialize(): # Run your one-time initialization process here\n",
    "        initialize_groq_environment()\n",
    "    try:\n",
    "        start_time = time.time()  # Record the start time\n",
    "        timestamped_print(\"Starting the HPO term extraction process.\")\n",
    "\n",
    "        # Check for temporary files at the very beginning\n",
    "        temp_files = [\n",
    "            'temp_combined_results.pkl',\n",
    "            'temp_exact_matches.pkl',\n",
    "            'temp_non_exact_matches.pkl',\n",
    "            'temp_final_result.pkl',\n",
    "            'responses_backup.pkl'\n",
    "        ]\n",
    "        temp_files_exist = any(os.path.exists(f) for f in temp_files)\n",
    "\n",
    "        if temp_files_exist:\n",
    "            timestamped_print(\"Temporary files found from a previous run. Attempting to resume processing.\")\n",
    "\n",
    "            # Load existing results if they exist\n",
    "            combined_results_df = pd.read_pickle('temp_combined_results.pkl') if os.path.exists('temp_combined_results.pkl') else pd.DataFrame()\n",
    "            exact_matches_df = pd.read_pickle('temp_exact_matches.pkl') if os.path.exists('temp_exact_matches.pkl') else pd.DataFrame()\n",
    "            non_exact_matches_df = pd.read_pickle('temp_non_exact_matches.pkl') if os.path.exists('temp_non_exact_matches.pkl') else pd.DataFrame()\n",
    "            final_result_df = pd.read_pickle('temp_final_result.pkl') if os.path.exists('temp_final_result.pkl') else pd.DataFrame()\n",
    "            responses_df = pd.read_pickle('responses_backup.pkl') if os.path.exists('responses_backup.pkl') else pd.DataFrame()\n",
    "\n",
    "            # Debugging statements to check DataFrame shapes\n",
    "            timestamped_print(f\"Combined results DataFrame shape: {combined_results_df.shape}\")\n",
    "            timestamped_print(f\"Exact matches DataFrame shape: {exact_matches_df.shape}\")\n",
    "            timestamped_print(f\"Non-exact matches DataFrame shape: {non_exact_matches_df.shape}\")\n",
    "            timestamped_print(f\"Final results DataFrame shape: {final_result_df.shape}\")\n",
    "            timestamped_print(f\"Responses DataFrame shape: {responses_df.shape if 'responses_df' in locals() else 'Not loaded'}\")\n",
    "\n",
    "            # Determine which steps need to be executed based on which temp files are missing\n",
    "            steps_to_run = []\n",
    "\n",
    "            if combined_results_df.empty:\n",
    "                steps_to_run.append('process_clinical_notes')\n",
    "            if exact_matches_df.empty or non_exact_matches_df.empty:\n",
    "                steps_to_run.append('process_exact_non_exact_matches')\n",
    "            if not non_exact_matches_df.empty and ('HPO_Term' not in non_exact_matches_df.columns or non_exact_matches_df['HPO_Term'].isna().any()):\n",
    "                steps_to_run.append('generate_hpo_terms')\n",
    "            if final_result_df.empty:\n",
    "                steps_to_run.append('compile_final_results')\n",
    "\n",
    "            timestamped_print(f\"Steps to run: {steps_to_run}\")\n",
    "        else:\n",
    "            timestamped_print(\"No temporary files found. Starting a new processing run.\")\n",
    "\n",
    "            # Option for user input\n",
    "            user_input = input(\"Do you want to provide clinical notes directly (yes/no)? \").strip().lower()\n",
    "            if user_input == 'yes':\n",
    "                clinical_notes = []\n",
    "                while True:\n",
    "                    note = input(\"Enter clinical note (or type 'done' to finish): \").strip()\n",
    "                    if note.lower() == 'done':\n",
    "                        break\n",
    "                    clinical_notes.append(note)\n",
    "                df = pd.DataFrame({'clinical_note': clinical_notes})\n",
    "\n",
    "                # Assign patient IDs if not provided\n",
    "                df['patient_id'] = range(1, len(df) + 1)\n",
    "            else:\n",
    "                while True:\n",
    "                    input_file = input(\"Enter the filename of the CSV containing clinical notes: \").strip()\n",
    "                    if input_file.lower().endswith('.csv'):\n",
    "                        try:\n",
    "                            df = pd.read_csv(input_file)\n",
    "\n",
    "                            # Check for 'patient_id' or 'case number' columns\n",
    "                            if 'patient_id' in df.columns:\n",
    "                                df['patient_id'] = df['patient_id']\n",
    "                            elif 'case number' in df.columns:\n",
    "                                df['patient_id'] = df['case number']\n",
    "                            else:\n",
    "                                # Assign new patient IDs if not present\n",
    "                                df['patient_id'] = range(1, len(df) + 1)\n",
    "\n",
    "                            break\n",
    "                        except FileNotFoundError:\n",
    "                            print(\"File not found. Please ensure the file exists and the path is correct.\")\n",
    "                        except pd.errors.EmptyDataError:\n",
    "                            print(\"The file is empty. Please provide a valid CSV file with data.\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred: {e}\")\n",
    "                    else:\n",
    "                        print(\"The file must have a .csv extension. Please provide a valid CSV file.\")\n",
    "\n",
    "            df = df.dropna(subset=['clinical_note'])  # Remove rows where 'clinical_note' is NaN\n",
    "            df['clinical_note'] = df['clinical_note'].astype(str)  # Ensure 'clinical_note' column is of type string\n",
    "\n",
    "            # Initialize empty DataFrames\n",
    "            combined_results_df = pd.DataFrame()\n",
    "            exact_matches_df = pd.DataFrame()\n",
    "            non_exact_matches_df = pd.DataFrame()\n",
    "            responses_df = pd.DataFrame()\n",
    "            final_result_df = pd.DataFrame()\n",
    "\n",
    "            steps_to_run = ['process_clinical_notes', 'process_exact_non_exact_matches', 'generate_hpo_terms', 'compile_final_results']\n",
    "\n",
    "        # Proceed with processing based on steps_to_run\n",
    "        if 'process_clinical_notes' in steps_to_run:\n",
    "            timestamped_print(\"Processing clinical notes.\")\n",
    "\n",
    "            # Initialize models and data\n",
    "            timestamped_print(\"Initializing embeddings model\")\n",
    "            embeddings_model = initialize_embeddings_model()\n",
    "            timestamped_print(\"Loading embedded documents\")\n",
    "            embedded_documents = load_embedded_documents('G2GHPO_metadata.npy')\n",
    "            timestamped_print(\"Preparing embeddings list\")\n",
    "            embeddings_array = prepare_embeddings_list(embedded_documents)\n",
    "            timestamped_print(\"Creating FAISS index\")\n",
    "            index = create_faiss_index(embeddings_array)\n",
    "\n",
    "            # Determine which patient_ids still need to be processed\n",
    "            if not combined_results_df.empty:\n",
    "                processed_patient_ids = set(combined_results_df['patient_id'].unique())\n",
    "            else:\n",
    "                processed_patient_ids = set()\n",
    "\n",
    "            all_patient_ids = set(df['patient_id'].unique())\n",
    "            remaining_patient_ids = all_patient_ids - processed_patient_ids\n",
    "\n",
    "            if remaining_patient_ids:\n",
    "                timestamped_print(f\"Processing {len(remaining_patient_ids)} new clinical notes.\")\n",
    "\n",
    "                # Process each remaining clinical note\n",
    "                for _, row in df[df['patient_id'].isin(remaining_patient_ids)].iterrows():\n",
    "                    clinical_note = row['clinical_note']\n",
    "                    patient_id = row['patient_id']\n",
    "                    timestamped_print(f\"Processing clinical note for patient_id {patient_id}: {clinical_note[:30]}...\")\n",
    "                    result_df = process_row(clinical_note, system_message_I, embeddings_model, index, embedded_documents)\n",
    "                    if result_df is not None:\n",
    "                        result_df['patient_id'] = patient_id\n",
    "                        combined_results_df = pd.concat([combined_results_df, result_df], ignore_index=True)\n",
    "                    time.sleep(2)\n",
    "                    combined_results_df.to_pickle('temp_combined_results.pkl')\n",
    "            else:\n",
    "                timestamped_print(\"All clinical notes have been processed in 'temp_combined_results.pkl'.\")\n",
    "\n",
    "        else:\n",
    "            timestamped_print(\"Combined results loaded from 'temp_combined_results.pkl'.\")\n",
    "\n",
    "        if combined_results_df.empty:\n",
    "            timestamped_print(\"No data in combined_results_df. Exiting.\")\n",
    "            sys.exit(0)\n",
    "\n",
    "        combined_results_df['phrase'] = combined_results_df['phrase'].str.lower()\n",
    "        combined_results_df['unique_metadata'] = combined_results_df['unique_metadata'].apply(process_unique_metadata)\n",
    "\n",
    "        if 'process_exact_non_exact_matches' in steps_to_run:\n",
    "            timestamped_print(\"Processing exact and non-exact matches.\")\n",
    "\n",
    "            # Add HPO terms for exact matches\n",
    "            combined_results_df['HPO_Term'] = combined_results_df.apply(\n",
    "                lambda row: extract_hpo_term(row['phrase'], row['unique_metadata']), axis=1\n",
    "            )\n",
    "\n",
    "            # Separate exact and non-exact matches\n",
    "            exact_matches_df = combined_results_df.dropna(subset=['HPO_Term'])\n",
    "            non_exact_matches_df = combined_results_df[combined_results_df['HPO_Term'].isna()]\n",
    "            exact_matches_df.to_pickle('temp_exact_matches.pkl')\n",
    "            non_exact_matches_df.to_pickle('temp_non_exact_matches.pkl')\n",
    "        else:\n",
    "            timestamped_print(\"Exact and non-exact matches loaded from temporary files.\")\n",
    "\n",
    "        # Ensure 'HPO_Term' column exists and is of correct type\n",
    "        if 'HPO_Term' in non_exact_matches_df.columns:\n",
    "            # Identify indices where 'HPO_Term' is NaN (unprocessed entries)\n",
    "            remaining_indices = non_exact_matches_df[non_exact_matches_df['HPO_Term'].isna()].index\n",
    "            timestamped_print(f\"Number of unprocessed non-exact matches: {len(remaining_indices)}\")\n",
    "        else:\n",
    "            remaining_indices = non_exact_matches_df.index\n",
    "            # Initialize 'HPO_Term' column with NaN values\n",
    "            non_exact_matches_df['HPO_Term'] = np.nan\n",
    "            timestamped_print(f\"Total non-exact matches to process: {len(remaining_indices)}\")\n",
    "\n",
    "        # Check if non_exact_matches_df is empty\n",
    "        if non_exact_matches_df.empty:\n",
    "            timestamped_print(\"No non-exact matches found. Skipping HPO term generation for non-exact matches.\")\n",
    "            # Ensure 'HPO_Term' column exists\n",
    "            if 'HPO_Term' not in non_exact_matches_df.columns:\n",
    "                non_exact_matches_df['HPO_Term'] = pd.Series(dtype=\"object\")\n",
    "            # Remove 'generate_hpo_terms' from steps_to_run if present\n",
    "            if 'generate_hpo_terms' in steps_to_run:\n",
    "                steps_to_run.remove('generate_hpo_terms')\n",
    "\n",
    "        if 'generate_hpo_terms' in steps_to_run:\n",
    "            timestamped_print(\"Generating HPO terms for non-exact matches.\")\n",
    "            timestamped_print(f\"Processing {len(remaining_indices)} non-exact matches.\")\n",
    "\n",
    "            # Initialize responses DataFrame if empty\n",
    "            if responses_df.empty:\n",
    "                responses_df = pd.DataFrame(columns=['response'])\n",
    "\n",
    "            # Process the remaining non-exact matches\n",
    "            if len(remaining_indices) > 0:\n",
    "                for idx in tqdm(remaining_indices, total=len(remaining_indices)):\n",
    "                    row = non_exact_matches_df.loc[idx]\n",
    "                    response_df = generate_hpo_terms(\n",
    "                        pd.DataFrame([row]),\n",
    "                        system_message_II\n",
    "                    )\n",
    "                    if response_df['response'].iloc[0] is not None:\n",
    "                        responses_df = pd.concat([responses_df, response_df], ignore_index=True)\n",
    "                        non_exact_matches_df.at[idx, 'HPO_Term'] = response_df['response'].iloc[0]\n",
    "                    else:\n",
    "                        # Handle cases where response is None due to repeated failures\n",
    "                        non_exact_matches_df.at[idx, 'HPO_Term'] = 'Error: Unable to process'\n",
    "                    time.sleep(2)\n",
    "                    if idx % 10 == 0:\n",
    "                        responses_df.to_pickle('responses_backup.pkl')\n",
    "                        non_exact_matches_df.to_pickle('temp_non_exact_matches.pkl')\n",
    "                # Save final progress\n",
    "                responses_df.to_pickle('responses_backup.pkl')\n",
    "                non_exact_matches_df.to_pickle('temp_non_exact_matches.pkl')\n",
    "            else:\n",
    "                timestamped_print(\"No unprocessed non-exact matches found.\")\n",
    "        else:\n",
    "            timestamped_print(\"All non-exact matches have been processed.\")\n",
    "\n",
    "        if 'compile_final_results' in steps_to_run:\n",
    "            timestamped_print(\"Compiling final results.\")\n",
    "\n",
    "            # Combine results and prepare final output\n",
    "            final_combined_df = pd.concat([exact_matches_df, non_exact_matches_df], ignore_index=True)\n",
    "            final_combined_df_grouped = final_combined_df.groupby('patient_id').apply(\n",
    "                lambda group: group[['phrase', 'HPO_Term']].to_dict('records')\n",
    "            )\n",
    "\n",
    "            final_result_df = pd.DataFrame({\n",
    "                'patient_id': final_combined_df_grouped.index,\n",
    "                'HPO_Terms': final_combined_df_grouped.values\n",
    "            })\n",
    "            final_result_df.to_pickle('temp_final_result.pkl')\n",
    "        else:\n",
    "            timestamped_print(\"Final results loaded from 'temp_final_result.pkl'.\")\n",
    "\n",
    "        # Process the final results\n",
    "        process_results(final_result_df) \n",
    "\n",
    "        timestamped_print(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        # Delete temporary files\n",
    "        for temp_file in temp_files:\n",
    "            if os.path.exists(temp_file):\n",
    "                os.remove(temp_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        timestamped_print(f\"An unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
