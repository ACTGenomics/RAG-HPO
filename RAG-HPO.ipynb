{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d96edb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-29 00:33:34 - Starting HPO extraction pipeline...\n",
      "2025-07-29 00:33:34 - Loaded checkpoint 'input' (114 rows)\n",
      "2025-07-29 00:33:34 - Loaded checkpoint 'combined' (1143 rows)\n",
      "2025-07-29 00:33:34 - Loaded checkpoint 'exact' (356 rows)\n",
      "2025-07-29 00:33:34 - Loaded checkpoint 'non_exact' (787 rows)\n",
      "2025-07-29 00:33:34 - No checkpoint found for 'final', starting fresh.\n",
      "2025-07-29 00:33:34 - No checkpoint found for 'responses', starting fresh.\n",
      "2025-07-29 00:33:34 - Resuming from existing 'combined' checkpoint.\n",
      "2025-07-29 00:33:37 - Skipped clinical processing; checkpoint exists.\n",
      "2025-07-29 00:33:37 - Skipped splitting; checkpoints exist.\n",
      "2025-07-29 00:33:37 - Generating HPO for 487 entries...\n",
      "2025-07-29 00:33:37 - Skipped clinical processing; checkpoint exists.\n",
      "2025-07-29 00:33:37 - Skipped splitting; checkpoints exist.\n",
      "2025-07-29 00:33:37 - Generating HPO for 487 entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating HPO:  10%|█         | 50/487 [01:52<18:16,  2.51s/entry]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-29 00:35:30 - [SAVE] Checkpointed 'non_exact' (787 rows) -> /Users/garciabt/Library/CloudStorage/OneDrive-BaylorCollegeofMedicine/01 Posey Lab/RAG-HPO/temp_non_exact_matches.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating HPO: 100%|██████████| 487/487 [18:27<00:00,  2.27s/entry]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-29 00:52:05 - Compiling final results...\n",
      "2025-07-29 00:52:05 - [SAVE] Checkpointed 'final' (114 rows) -> /Users/garciabt/Library/CloudStorage/OneDrive-BaylorCollegeofMedicine/01 Posey Lab/RAG-HPO/temp_final_result.pkl\n",
      "2025-07-29 00:52:23 - Saved tabular results to GSC-LLAMA4-II.csv\n",
      "2025-07-29 00:52:23 - Saved raw JSON results to GSC-LLAMA4-II_json_raw.csv\n",
      "2025-07-29 00:52:23 - Pipeline completed in 1129.35s\n",
      "2025-07-29 00:52:23 - Pipeline succeeded. Cleaning up temporary files...\n",
      "2025-07-29 00:52:23 - Saved tabular results to GSC-LLAMA4-II.csv\n",
      "2025-07-29 00:52:23 - Saved raw JSON results to GSC-LLAMA4-II_json_raw.csv\n",
      "2025-07-29 00:52:23 - Pipeline completed in 1129.35s\n",
      "2025-07-29 00:52:23 - Pipeline succeeded. Cleaning up temporary files...\n"
     ]
    }
   ],
   "source": [
    "# ======================= Imports =======================\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "import subprocess\n",
    "from contextlib import contextmanager\n",
    "import shutil\n",
    "import traceback\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import faiss\n",
    "from typing import List, Dict, Any\n",
    "from fastembed import TextEmbedding\n",
    "from rapidfuzz import fuzz as rfuzz\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ======================= Global Constants =======================\n",
    "FLAG_FILE = \"LLaMa-4-Scout.flag\"\n",
    "TEMP_FILES = {\n",
    "    'input':    'temp_input.pkl',\n",
    "    'combined': 'temp_combined_results.pkl',\n",
    "    'exact':    'temp_exact_matches.pkl',\n",
    "    'non_exact':'temp_non_exact_matches.pkl',\n",
    "    'final':    'temp_final_result.pkl',\n",
    "    'responses':'responses_backup.pkl'\n",
    "}\n",
    "\n",
    "# ======================= Logger Class =======================\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.printed_messages = set()\n",
    "\n",
    "    def log(self, msg, once=False):\n",
    "        \"\"\"Prints a timestamped message. If once=True, message is only printed once per session.\"\"\"\n",
    "        if once:\n",
    "            # Use a hash of the message to check for duplicates\n",
    "            msg_hash = hash(msg)\n",
    "            if msg_hash in self.printed_messages:\n",
    "                return\n",
    "            self.printed_messages.add(msg_hash)\n",
    "        print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - {msg}\")\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "# ======================= LLM Client =======================\n",
    "class LLMClient:\n",
    "    def __init__(self, api_key, base_url, model_name=\"llama3-groq-70b-8192-tool-use-preview\", max_tokens_per_day=500000, max_queries_per_minute=30, temperature=0.7):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model_name = model_name\n",
    "        self.max_tokens_per_day = max_tokens_per_day\n",
    "        self.max_queries_per_minute = max_queries_per_minute\n",
    "        self.temperature = temperature\n",
    "        self.total_tokens_used = 0\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        try:\n",
    "            self.encoder = tiktoken.encoding_for_model(self.model_name)\n",
    "        except KeyError:\n",
    "            self.encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def query(self, user_input, system_message):\n",
    "        # Sends a query to the LLM API and tracks token usage\n",
    "        tokens_ui = len(self.encoder.encode(user_input))\n",
    "        tokens_sys = len(self.encoder.encode(system_message))\n",
    "        estimated = tokens_ui + tokens_sys\n",
    "        if self.total_tokens_used + estimated > self.max_tokens_per_day:\n",
    "            raise Exception(\"Token limit exceeded for the day.\")\n",
    "        time.sleep(60 / self.max_queries_per_minute)\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\",   \"content\": user_input}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "        resp = requests.post(self.base_url, headers=self.headers, json=payload)\n",
    "        resp.raise_for_status()\n",
    "        result = resp.json()\n",
    "        if \"usage\" in result and \"total_tokens\" in result[\"usage\"]:\n",
    "            self.total_tokens_used += result[\"usage\"][\"total_tokens\"]\n",
    "        else:\n",
    "            self.total_tokens_used += estimated\n",
    "        choices = result.get(\"choices\") or []\n",
    "        return choices[0].get(\"message\", {}).get(\"content\", \"\") if choices else \"\"\n",
    "\n",
    "# ======================= Prompt Loading =======================\n",
    "def load_prompts(file_path=\"system_prompts.json\"):\n",
    "    if not os.path.exists(file_path):\n",
    "        logger.log(f\"Error: Prompt file '{file_path}' not found.\")\n",
    "        sys.exit(1)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "prompts = load_prompts()\n",
    "system_message_I = prompts.get(\"system_message_I\", \"\")\n",
    "system_message_II = prompts.get(\"system_message_II\", \"\")\n",
    "\n",
    "# ======================= Environment Setup =======================\n",
    "llm_client = None\n",
    "\n",
    "def check_and_initialize():\n",
    "    global llm_client\n",
    "    if os.path.exists(FLAG_FILE):\n",
    "        with open(FLAG_FILE, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        llm_client = LLMClient(\n",
    "            api_key=config[\"api_key\"],\n",
    "            base_url=config.get(\"base_url\", \"\"),\n",
    "            model_name=config.get(\"model_name\", \"\"),\n",
    "            max_tokens_per_day=config.get(\"max_tokens_per_day\", 500000),\n",
    "            max_queries_per_minute=config.get(\"max_queries_per_minute\", 30),\n",
    "            temperature=config.get(\"temperature\", 0.7)\n",
    "        )\n",
    "        return True\n",
    "    else:\n",
    "        initialize_groq_environment()\n",
    "        return False\n",
    "\n",
    "def initialize_groq_environment():\n",
    "    global llm_client\n",
    "    api_key = input(\"Enter your API key (required): \").strip()\n",
    "    base_url = input(\"Enter base URL (default https://api.groq.com/openai/v1/chat/completions): \") or \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    model_name = input(\"Enter model name (default llama3-groq-70b-8192-tool-use-preview): \") or \"llama3-groq-70b-8192-tool-use-preview\"\n",
    "    max_tokens = input(\"Max tokens/day (default 500000): \")\n",
    "    max_tokens = int(max_tokens) if max_tokens else 500000\n",
    "    max_qpm = input(\"Max queries/minute (default 30): \")\n",
    "    max_qpm = int(max_qpm) if max_qpm else 30\n",
    "    temp = input(\"Temperature (0.0–1.0, default 0.2): \")\n",
    "    try:\n",
    "        temp = float(temp) if temp else 0.2\n",
    "    except ValueError:\n",
    "        temp = 0.7\n",
    "    llm_client = LLMClient(api_key, base_url, model_name, max_tokens, max_qpm, temp)\n",
    "    with open(FLAG_FILE, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"api_key\": api_key,\n",
    "            \"base_url\": base_url,\n",
    "            \"model_name\": model_name,\n",
    "            \"max_tokens_per_day\": max_tokens,\n",
    "            \"max_queries_per_minute\": max_qpm,\n",
    "            \"temperature\": temp\n",
    "        }, f)\n",
    "\n",
    "# ======================= Subprocess & Logging =======================\n",
    "@contextmanager\n",
    "def managed_subprocess(*args, **kwargs):\n",
    "    proc = subprocess.Popen(*args, **kwargs)\n",
    "    try:\n",
    "        yield proc\n",
    "    finally:\n",
    "        proc.terminate()\n",
    "        proc.wait()\n",
    "\n",
    "def timestamped_print(msg):\n",
    "    logger.log(msg)\n",
    "\n",
    "# ======================= Embeddings & FAISS =======================\n",
    "PAT = re.compile(r'\\(.*?\\)')\n",
    "\n",
    "def clean_text(txt: str) -> str:\n",
    "    return PAT.sub('', txt or '').replace('_', ' ').lower().strip()\n",
    "\n",
    "def initialize_embeddings_model(use_sbert: bool = True, sbert_model: str = 'pritamdeka/SapBERT-mnli-snli-scinli-scitail-mednli-stsb', bge_model: str = 'BAAI/bge-small-en-v1.5'):\n",
    "    try:\n",
    "        if use_sbert:\n",
    "            return SentenceTransformer(sbert_model)\n",
    "        return TextEmbedding(model_name=bge_model)\n",
    "    except Exception as e:\n",
    "        logger.log(f\"[FATAL] Could not initialize embedding model: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def load_vector_db(meta_path: str = 'hpo_meta.json',\n",
    "                   vec_path:  str = 'hpo_embedded.npz'):\n",
    "    # ─── Sanity checks ───\n",
    "    if not os.path.exists(meta_path) or not os.path.exists(vec_path):\n",
    "        logger.log(f\"[FATAL] DB files not found: {meta_path}, {vec_path}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # ─── Load the condensed JSON ───\n",
    "    try:\n",
    "        with open(meta_path, 'r', encoding='utf-8') as f:\n",
    "            combined = json.load(f)\n",
    "            constants = combined.get('constants', {})\n",
    "            entries  = combined.get('entries', [])\n",
    "    except Exception as e:\n",
    "        logger.log(f\"[FATAL] Could not load metadata JSON: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # ─── Load the embeddings ───\n",
    "    try:\n",
    "        arr = np.load(vec_path)\n",
    "        emb_matrix = arr['emb'].astype(np.float32)\n",
    "    except Exception as e:\n",
    "        logger.log(f\"[FATAL] Could not load embedding npz: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # ─── Warn if lengths mismatch ───\n",
    "    if len(entries) != emb_matrix.shape[0]:\n",
    "        logger.log(\"[WARN] Metadata entries count and embedding rows mismatch \"\n",
    "              f\"({len(entries)} vs {emb_matrix.shape[0]})\")\n",
    "\n",
    "    # ─── Reconstruct docs list in the original output format ───\n",
    "    docs = []\n",
    "    for entry, vec in zip(entries, emb_matrix):\n",
    "        hp_id = entry.get('hp_id')\n",
    "        const = constants.get(hp_id, {})\n",
    "\n",
    "        doc = {\n",
    "            'hp_id':          hp_id,\n",
    "            'info':           entry.get('info'),\n",
    "            'lineage':        const.get('lineage'),\n",
    "            'organ_system':   const.get('organ_system'),\n",
    "            'direction':      entry.get('direction'),\n",
    "            # preserve these keys even if absent in the new JSON:\n",
    "            'depth':          const.get('depth'),\n",
    "            'parent_count':   const.get('parent_count'),\n",
    "            'child_count':    const.get('child_count'),\n",
    "            'descendant_count': const.get('descendant_count'),\n",
    "            'embedding':      vec\n",
    "        }\n",
    "        docs.append(doc)\n",
    "\n",
    "    return docs, emb_matrix\n",
    "def create_faiss_index(emb_matrix: np.ndarray, metric: str = 'cosine'):\n",
    "    # Build FAISS index for embeddings\n",
    "    dim = emb_matrix.shape[1]\n",
    "    if metric == 'cosine':\n",
    "        faiss.normalize_L2(emb_matrix)\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "    index.add(emb_matrix)\n",
    "    return index\n",
    "\n",
    "def embed_query(text: str, model, metric: str = 'cosine'):\n",
    "    if hasattr(model, 'encode'):\n",
    "        vec = model.encode(text, convert_to_numpy=True)\n",
    "    else:\n",
    "        vec = np.array(list(model.embed([text]))[0], dtype=np.float32)\n",
    "    if vec.ndim == 1:\n",
    "        vec = vec.reshape(1, -1)\n",
    "    if metric == 'cosine':\n",
    "        faiss.normalize_L2(vec)\n",
    "    return vec\n",
    "\n",
    "def clean_note(text: str) -> str:\n",
    "    # Fix typical encoding issues\n",
    "    text = text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "    # Normalize unicode (e.g., smart quotes)\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    # Remove non-ASCII characters (optional: keep certain ones like µ or – if needed)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # Remove multiple spaces and trim\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# ======================= Phenotype Processing =======================\n",
    "def _collect_metadata_best(\n",
    "    phrase: str,\n",
    "    query_vec: np.ndarray,\n",
    "    index: faiss.Index,\n",
    "    docs: List[Dict[str, Any]],\n",
    "    top_k: int = 500,\n",
    "    similarity_threshold: float = 0.35,\n",
    "    min_unique: int = 15,\n",
    "    max_unique: int = 20\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Single‐pass hybrid retrieval: token overlap, threshold, fill to min_unique.\n",
    "    \"\"\"\n",
    "    clean_tokens = set(re.findall(r'\\w+', phrase.lower()))\n",
    "    dists, idxs = index.search(query_vec, top_k)\n",
    "    sims, indices = dists[0], idxs[0]\n",
    "\n",
    "    seen_hp = set()\n",
    "    results = []\n",
    "\n",
    "    for sim, idx in sorted(zip(sims, indices), key=lambda x: x[0], reverse=True):\n",
    "        if len(results) >= max_unique:\n",
    "            break\n",
    "        doc = docs[idx]\n",
    "        hp = doc.get('hp_id')\n",
    "        if not hp or hp in seen_hp:\n",
    "            continue\n",
    "\n",
    "        info = doc.get('info', '') or ''\n",
    "        token_overlap = bool(clean_tokens & set(re.findall(r'\\w+', info.lower())))\n",
    "\n",
    "        # accept if token overlap, or above similarity threshold, or to reach min_unique\n",
    "        if token_overlap or sim >= similarity_threshold or len(results) < min_unique:\n",
    "            seen_hp.add(hp)\n",
    "            results.append({\n",
    "                'hp_id': hp,\n",
    "                'phrase': info,\n",
    "                'definition': doc.get('definition'),\n",
    "                'organ_system': doc.get('organ_system'),\n",
    "                'similarity': float(sim)\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "def split_exact_nonexact(df: pd.DataFrame, hpo_term_col='HPO_Term'):\n",
    "    # print(f\">> DEBUG: about to split, columns: {df.columns.tolist()}\")\n",
    "    if 'category' not in df.columns:\n",
    "        raise KeyError(f\"[FATAL] 'category' missing; columns: {df.columns.tolist()}\")\n",
    "    if hpo_term_col not in df.columns:\n",
    "        raise KeyError(f\"[FATAL] '{hpo_term_col}' missing; columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # only keep Abnormal rows\n",
    "    df_ab = df[df['category'] == 'Abnormal']\n",
    "    # print(f\">> DEBUG: {len(df_ab)} rows with category=='Abnormal'\")\n",
    "\n",
    "    exact_df     = df_ab.dropna(subset=[hpo_term_col]).copy()\n",
    "    non_exact_df = df_ab[df_ab[hpo_term_col].isna()].copy()\n",
    "    # print(f\">> DEBUG: exact_df.shape={exact_df.shape}, non_exact_df.shape={non_exact_df.shape}\")\n",
    "    return exact_df, non_exact_df\n",
    "\n",
    "def process_findings(findings, clinical_note: str, embeddings_model, index, docs,\n",
    "                    metric: str = 'cosine',\n",
    "                    keep_top: int = 15):\n",
    "    \"\"\"\n",
    "    Processes findings and returns DataFrame with phrase, category,\n",
    "    metadata, sentence, patient_id.\n",
    "    - keep_top: number of unique metadata entries to retrieve.\n",
    "    \"\"\"\n",
    "    # ─── Position A: Split note into sentences for context matching ───\n",
    "    sentences = [s.strip() for s in clinical_note.split('.') if s.strip()]\n",
    "    rows = []\n",
    "\n",
    "    for f in findings:\n",
    "        phrase = f.get('phrase', '').strip()\n",
    "        category = f.get('category', '')\n",
    "        if not phrase:\n",
    "            continue\n",
    "\n",
    "        # ─── Position B: Embed the phrase ───\n",
    "        qv = embed_query(phrase, embeddings_model, metric=metric)\n",
    "\n",
    "        # ─── Position C: Retrieve best metadata candidates ───\n",
    "        unique_metadata = _collect_metadata_best(\n",
    "            phrase=phrase,               # literal text for token-phase\n",
    "            query_vec=qv,                # embedded vector\n",
    "            index=index,                 # FAISS index\n",
    "            docs=docs,                   # list of HPO docs\n",
    "            top_k=500,                   # FAISS retrieval size\n",
    "            similarity_threshold=0.35,   # max distance for semantic matches\n",
    "            min_unique=keep_top,         # ensure at least keep_top entries\n",
    "            max_unique=keep_top          # cap at keep_top entries\n",
    "        )\n",
    "\n",
    "        # ─── Position D: Find the best-matching sentence ───\n",
    "        fw = set(re.findall(r'\\b\\w+\\b', phrase.lower()))\n",
    "        best_sent, best_score = None, 0\n",
    "        for s in sentences:\n",
    "            sw = set(re.findall(r'\\b\\w+\\b', s.lower()))\n",
    "            score = len(fw & sw)\n",
    "            if score > best_score:\n",
    "                best_score, best_sent = score, s\n",
    "\n",
    "        # ─── Position E: Collect row ───\n",
    "        rows.append({\n",
    "            'phrase':           phrase,\n",
    "            'category':         category,\n",
    "            'unique_metadata':  unique_metadata,\n",
    "            'original_sentence': best_sent,\n",
    "            'patient_id':       f.get('patient_id')\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def clean_and_parse(s: str):\n",
    "    # Extracts and parses JSON from string\n",
    "    try:\n",
    "        m = re.search(r'\\{.*\\}', s, flags=re.S)\n",
    "        js_str = m.group(0) if m else s.strip()\n",
    "        return json.loads(js_str)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_findings(response: str) -> list:\n",
    "    # Extracts findings from LLM response\n",
    "    if not response:\n",
    "        return []\n",
    "    parsed = clean_and_parse(response)\n",
    "    if not isinstance(parsed, dict):\n",
    "        return []\n",
    "    return parsed.get(\"phenotypes\", [])\n",
    "\n",
    "# ======================= Single-Row Processing =======================\n",
    "def process_row(clinical_note, system_message, embeddings_model, index, embedded_documents):\n",
    "    # Clean the clinical note before sending to the LLM\n",
    "    clinical_note = clean_note(clinical_note)\n",
    "    # Query the LLM\n",
    "    raw = llm_client.query(clinical_note, system_message)\n",
    "    # print(f\"LLM response: {raw}...\")  # Print first 1000 chars for debugging\n",
    "\n",
    "    # Try to parse findings robustly\n",
    "    findings = extract_findings(raw)\n",
    "    # If findings is empty, try to parse as a list of dicts (sometimes LLMs return just a list)\n",
    "    if not findings:\n",
    "        try:\n",
    "            parsed = json.loads(raw)\n",
    "            if isinstance(parsed, list):\n",
    "                findings = parsed\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # If still empty, try to extract any dicts with 'phrase' and 'category' keys\n",
    "    if not findings:\n",
    "        try:\n",
    "            matches = re.findall(r'\\{[^\\}]*\\}', raw)\n",
    "            findings = []\n",
    "            for m in matches:\n",
    "                try:\n",
    "                    d = json.loads(m)\n",
    "                    if 'phrase' in d and 'category' in d:\n",
    "                        findings.append(d)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    findings = [f for f in findings if isinstance(f, dict) and f.get('category') == 'Abnormal']\n",
    "\n",
    "    # Return empty DataFrame if no valid findings, but with required columns\n",
    "    required_cols = ['phrase', 'category', 'unique_metadata', 'original_sentence', 'patient_id']\n",
    "    if not findings:\n",
    "        return pd.DataFrame(columns=required_cols)\n",
    "    # Continue processing\n",
    "    df = process_findings(findings, clinical_note, embeddings_model, index, embedded_documents)\n",
    "    # Ensure all required columns are present\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "    return df\n",
    "\n",
    "# ======================= HPO Term Extraction =======================\n",
    "def _iter_term_hp(item):\n",
    "    # Yields (term_text, hp_id) pairs from metadata entries\n",
    "    if isinstance(item, str):\n",
    "        try:\n",
    "            d = json.loads(item)\n",
    "        except Exception:\n",
    "            return\n",
    "    elif isinstance(item, dict):\n",
    "        d = item\n",
    "    else:\n",
    "        return\n",
    "    if \"hp_id\" in d:\n",
    "        term_text = d.get(\"info\") or d.get(\"label\")\n",
    "        hp = d[\"hp_id\"]\n",
    "        if hp and term_text:\n",
    "            yield term_text, hp\n",
    "        return\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, str) and v.startswith(\"HP:\"):\n",
    "            yield k, v\n",
    "\n",
    "def build_cluster_index(metadata_list):\n",
    "    # Builds cluster index for bag-of-words matching\n",
    "    idx = defaultdict(lambda: defaultdict(list))\n",
    "    for entry in metadata_list:\n",
    "        for term, hp in _iter_term_hp(entry):\n",
    "            ct = clean_text(term)\n",
    "            if not ct:\n",
    "                continue\n",
    "            toks = ct.split()\n",
    "            sig = \" \".join(sorted(toks))\n",
    "            idx[sig][len(toks)].append(hp)\n",
    "    return idx\n",
    "\n",
    "def extract_hpo_term(phrase, metadata_list, cluster_index):\n",
    "    # Maps phenotype phrase to best HPO term using multiple strategies\n",
    "    if not metadata_list or (isinstance(metadata_list, float) and pd.isna(metadata_list)):\n",
    "        return None\n",
    "    cp = clean_text(phrase)\n",
    "    if not cp:\n",
    "        return None\n",
    "    toks = cp.split()\n",
    "    sig = \" \".join(sorted(toks))\n",
    "    if sig in cluster_index and len(toks) in cluster_index[sig]:\n",
    "        return cluster_index[sig][len(toks)][0]\n",
    "    pairs = []\n",
    "    for entry in metadata_list:\n",
    "        for term, hp in _iter_term_hp(entry):\n",
    "            ct = clean_text(term)\n",
    "            if ct:\n",
    "                pairs.append((ct, hp))\n",
    "    pset = set(toks)\n",
    "    for ct, hp in pairs:\n",
    "        if set(ct.split()) == pset:\n",
    "            return hp\n",
    "    for ct, hp in pairs:\n",
    "        if ct == cp:\n",
    "            return hp\n",
    "    if len(pset) > 1:\n",
    "        for ct, hp in pairs:\n",
    "            if re.search(rf\"\\b{re.escape(ct)}\\b\", cp):\n",
    "                return hp\n",
    "    best_hp, best_score = None, 0\n",
    "    for ct, hp in pairs:\n",
    "        score = rfuzz.token_sort_ratio(cp, ct)\n",
    "        if score > best_score:\n",
    "            best_hp, best_score = hp, score\n",
    "    return best_hp if best_score >= 80 else None\n",
    "\n",
    "def parse_llm_mapping(resp_text: str, candidate_ids: set) -> (str, str, dict):\n",
    "    \"\"\"\n",
    "    Simplified parsing: strict JSON → key lookup → regex fallback.\n",
    "    \"\"\"\n",
    "    # 1. Strict JSON parse\n",
    "    try:\n",
    "        js = json.loads(resp_text)\n",
    "    except json.JSONDecodeError:\n",
    "        js = None\n",
    "\n",
    "    # 2. Look for an HPO ID in known keys\n",
    "    if isinstance(js, dict):\n",
    "        candidate = next(\n",
    "            (js[k].strip().strip('\"') for k in (\"hpo_id\", \"HPO_ID\", \"hp_id\", \"id\")\n",
    "             if isinstance(js.get(k), str)),\n",
    "            None\n",
    "        )\n",
    "        if candidate:\n",
    "            low = candidate.lower()\n",
    "            if low in (\"null\", \"no candidate fit\"):\n",
    "                return None, \"null_label\", js\n",
    "            if candidate in candidate_ids:\n",
    "                return candidate, \"ok\", js\n",
    "            return candidate, \"hp_not_in_candidates\", js\n",
    "\n",
    "    # 3. Regex fallback for HP:NNNNNNN\n",
    "    for m in re.findall(r\"(HP:\\d{6,7})\", resp_text):\n",
    "        if m in candidate_ids:\n",
    "            return m, \"regex_fallback\", None\n",
    "\n",
    "    # 4. Nothing found\n",
    "    return None, \"no_hpo_found\", None\n",
    "\n",
    "\n",
    "def generate_hpo_terms(df_row: pd.DataFrame, system_message: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Streamlined LLM + fallback logic with phrase normalization.\n",
    "    \"\"\"\n",
    "    # 1. Extract & normalize inputs\n",
    "    phrase = df_row['phrase'].iloc[0].strip()\n",
    "    normalized = phrase.lower().replace('-', ' ').strip()\n",
    "    category = df_row['category'].iloc[0]\n",
    "    original = df_row['original_sentence'].iloc[0]\n",
    "    metadata_list = df_row['unique_metadata'].iloc[0] or []\n",
    "\n",
    "    # 2. Build candidate list\n",
    "    candidates = []\n",
    "    seen = set()\n",
    "    for m in metadata_list:\n",
    "        term = m.get('phrase') or m.get('info')\n",
    "        hp = m.get('hp_id')\n",
    "        if term and hp and hp not in seen:\n",
    "            candidates.append({'term': term, 'id': hp})\n",
    "            seen.add(hp)\n",
    "    candidate_ids = {c['id'] for c in candidates}\n",
    "\n",
    "    # 3. Call LLM and parse\n",
    "    payload = json.dumps({\n",
    "        'phrase': normalized,\n",
    "        'category': category,\n",
    "        'original_sentence': original,\n",
    "        'candidates': candidates\n",
    "    })\n",
    "    resp = llm_client.query(payload, system_message)\n",
    "    hpo_id, reason, _ = parse_llm_mapping(resp, candidate_ids)\n",
    "\n",
    "    # 4. Local fallback if needed\n",
    "    if not hpo_id:\n",
    "        cluster_idx = build_cluster_index(metadata_list)\n",
    "        local_id = extract_hpo_term(normalized, metadata_list, cluster_idx)\n",
    "        if local_id:\n",
    "            hpo_id, reason = local_id, \"fallback_local\"\n",
    "\n",
    "    # 5. Return unified record\n",
    "    return pd.DataFrame([{\n",
    "        'HPO_Terms': [{'phrase': phrase, 'HPO_Term': hpo_id}],\n",
    "        'raw_llm_resp': resp,\n",
    "        'llm_parse_reason': reason\n",
    "    }])\n",
    "\n",
    "\n",
    "def standardize_input(df):\n",
    "    return validate_input(df)\n",
    "\n",
    "# ======================= Input Validation & State =======================\n",
    "def validate_input(df):\n",
    "    if 'clinical_note' not in df.columns:\n",
    "        raise KeyError(\"Missing required column: 'clinical_note'.\")\n",
    "    df = df.dropna(subset=['clinical_note']).copy()\n",
    "    df['clinical_note'] = df['clinical_note'].astype(str)\n",
    "    # Clean all clinical notes to prevent encoding-related bugs\n",
    "    df['clinical_note'] = df['clinical_note'].apply(clean_note)\n",
    "    if 'patient_id' not in df.columns:\n",
    "        df = df.reset_index(drop=True)\n",
    "        df['patient_id'] = df.index + 1\n",
    "    else:\n",
    "        df['patient_id'] = df['patient_id'].astype(int)\n",
    "    return df\n",
    "\n",
    "def load_state(temp_files):\n",
    "    # Loads pipeline state from temp files\n",
    "    state = {}\n",
    "    for key, path in temp_files.items():\n",
    "        try:\n",
    "            if os.path.exists(path):\n",
    "                df = pd.read_pickle(path)\n",
    "                state[key] = df\n",
    "                # This print is now handled in main\n",
    "            else:\n",
    "                state[key] = pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            state[key] = pd.DataFrame()\n",
    "            print(f\"Warning loading '{key}': {e}. Starting fresh for this key.\")\n",
    "    return state\n",
    "\n",
    "def process_results(final_df):\n",
    "    # Handles output of final results (CSV or display)\n",
    "    if final_df.empty:\n",
    "        logger.log(\"No final results to process.\")\n",
    "        return\n",
    "    choice = input(\"Save results as CSV or display? (save/display): \").strip().lower()\n",
    "    if choice == 'save':\n",
    "        fname = input(\"Output CSV filename (e.g., results.csv): \").strip()\n",
    "        if not fname:\n",
    "            print(\"Filename cannot be empty. Skipping save.\")\n",
    "            return\n",
    "        rows = []\n",
    "        for idx, r in final_df.iterrows():\n",
    "            pid = r.get('patient_id', idx)\n",
    "            for term in r.get('HPO_Terms', []):\n",
    "                ph = term.get('phrase', '').strip()\n",
    "                cat = term.get('category', '')\n",
    "                hp = term.get('HPO_Term') or ''\n",
    "                if isinstance(hp, str):\n",
    "                    hp = hp.replace(\"HP:HP:\", \"HP:\")\n",
    "                if hp:\n",
    "                    rows.append({\n",
    "                        'Patient ID': pid,\n",
    "                        'Category': cat,\n",
    "                        'Phenotype name': ph,\n",
    "                        'HPO ID': hp\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Warning: Blank HPO_Term for patient {pid}, phrase '{ph}', category '{cat}' - not included in CSV.\")\n",
    "        if rows:\n",
    "            output_df = pd.DataFrame(rows)\n",
    "            output_df.to_csv(fname, index=False)\n",
    "            logger.log(f\"Saved tabular results to {fname}\")\n",
    "        else:\n",
    "            logger.log(\"No valid HPO terms to save in tabular format.\")\n",
    "        final_df.to_csv(f\"{os.path.splitext(fname)[0]}_json_raw.csv\", index=False)\n",
    "        logger.log(f\"Saved raw JSON results to {os.path.splitext(fname)[0]}_json_raw.csv\")\n",
    "    elif choice == 'display':\n",
    "        tbl = []\n",
    "        for idx, r in final_df.iterrows():\n",
    "            pid = r.get('patient_id', idx)\n",
    "            for term in r.get('HPO_Terms', []):\n",
    "                ph = term.get('phrase', '').strip()\n",
    "                cat = term.get('category', '')\n",
    "                hp = term.get('HPO_Term') or ''\n",
    "                if isinstance(hp, str):\n",
    "                    hp = hp.replace(\"HP:HP:\", \"HP:\")\n",
    "                tbl.append({\n",
    "                    'Case': f\"Case {pid}\",\n",
    "                    'Category': cat,\n",
    "                    'Phenotype name': ph,\n",
    "                    'HPO ID': hp\n",
    "                })\n",
    "        if tbl:\n",
    "            print(tabulate(pd.DataFrame(tbl), headers='keys', tablefmt='psql'))\n",
    "        else:\n",
    "            logger.log(\"No terms to display.\")\n",
    "    else:\n",
    "        print(\"Invalid choice; please enter 'save' or 'display'.\")\n",
    "\n",
    "def cleanup(temp_files, success):\n",
    "    # Removes temp files if pipeline succeeded\n",
    "    if success:\n",
    "        logger.log(\"Pipeline succeeded. Cleaning up temporary files...\")\n",
    "        for path in temp_files.values():\n",
    "            try:\n",
    "                if os.path.exists(path):\n",
    "                    os.remove(path)\n",
    "                    # logger.log(f\"Removed temp file: {path}\")\n",
    "            except OSError as e:\n",
    "                logger.log(f\"Error removing temp file {path}: {e}\")\n",
    "    else:\n",
    "        logger.log(\"Pipeline failed. Keeping temporary files for debugging/resume.\")\n",
    "\n",
    "def save_state_checkpoint(state, temp_files, keys=('input','combined', 'exact', 'non_exact', 'final')):\n",
    "    # Saves pipeline state to disk\n",
    "    for key in keys:\n",
    "        df = state.get(key)\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        rel_path = temp_files.get(key)\n",
    "        if not rel_path:\n",
    "            logger.log(f\"Warning: No path configured for '{key}'. Skipping.\")\n",
    "            continue\n",
    "        abs_path = os.path.abspath(rel_path)\n",
    "        tmp_path = abs_path + \".tmp\"\n",
    "        try:\n",
    "            df.to_pickle(tmp_path)\n",
    "            shutil.move(tmp_path, abs_path)\n",
    "            logger.log(f\"[SAVE] Checkpointed '{key}' ({len(df)} rows) -> {abs_path}\", once=True)\n",
    "        except Exception as e:\n",
    "            logger.log(f\"Error saving '{key}': {e}\")\n",
    "            if os.path.exists(tmp_path): os.remove(tmp_path)\n",
    "\n",
    "# ======================= Main Pipeline =======================\n",
    "def main():\n",
    "    # 1) initialize environment\n",
    "    if not check_and_initialize():\n",
    "        print(\"Environment initialized.\")\n",
    "    timestamped_print(\"Starting HPO extraction pipeline...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 2) load checkpointed state\n",
    "    state = load_state(TEMP_FILES)\n",
    "    for key, df in state.items():\n",
    "        if not df.empty:\n",
    "            timestamped_print(f\"Loaded checkpoint '{key}' ({len(df)} rows)\")\n",
    "        else:\n",
    "            timestamped_print(f\"No checkpoint found for '{key}', starting fresh.\")\n",
    "\n",
    "    # 3) on first run, ingest notes\n",
    "    if state['combined'].empty:\n",
    "        if input(\"Manual notes? (yes/no): \").strip().lower() == 'yes':\n",
    "            notes = []\n",
    "            while True:\n",
    "                note = input(\"Note (or 'done'): \")\n",
    "                if note.lower() == 'done':\n",
    "                    break\n",
    "                notes.append(note)\n",
    "            df_input = pd.DataFrame({'clinical_note': notes})\n",
    "        else:\n",
    "            while True:\n",
    "                fname = input(\"CSV filename: \")\n",
    "                try:\n",
    "                    raw = pd.read_csv(fname)\n",
    "                    df_input = standardize_input(raw)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading CSV: {e}\")\n",
    "        state['input'] = validate_input(df_input)\n",
    "        save_state_checkpoint(state, TEMP_FILES, keys=['input'])\n",
    "    else:\n",
    "        timestamped_print(\"Resuming from existing 'combined' checkpoint.\")\n",
    "\n",
    "    # 4) initialize models and indices\n",
    "    emb_model    = initialize_embeddings_model(use_sbert=True)\n",
    "    docs, emb_matrix = load_vector_db(meta_path='hpo_meta.json', vec_path='hpo_embedded.npz')\n",
    "    index        = create_faiss_index(emb_matrix, metric='cosine')\n",
    "    cluster_index = build_cluster_index(docs)\n",
    "\n",
    "    success = False\n",
    "    try:\n",
    "        # 5) process raw clinical notes if needed\n",
    "        if state['combined'].empty:\n",
    "            timestamped_print(\"Processing clinical notes...\")\n",
    "            combined = pd.DataFrame()\n",
    "            pids     = sorted(state['input']['patient_id'].unique())\n",
    "            total    = len(pids)\n",
    "            for i, pid in enumerate(tqdm(pids, desc=\"Processing Notes\", unit=\"note\")):\n",
    "                note = state['input'].loc[state['input']['patient_id'] == pid, 'clinical_note'].iloc[0]\n",
    "                res  = process_row(note, system_message_I, emb_model, index, docs)\n",
    "                if not res.empty:\n",
    "                    res['patient_id'] = pid\n",
    "                    combined = pd.concat([combined, res], ignore_index=True)\n",
    "                # checkpoint every 20 notes\n",
    "                if (i + 1) % 20 == 0 or (i + 1) == total:\n",
    "                    state['combined'] = combined.copy()\n",
    "                    save_state_checkpoint(state, TEMP_FILES, keys=['combined'])\n",
    "            state['combined'] = combined.copy()\n",
    "            save_state_checkpoint(state, TEMP_FILES, keys=['combined'])\n",
    "        else:\n",
    "            timestamped_print(\"Skipped clinical processing; checkpoint exists.\")\n",
    "\n",
    "        # 6) split into exact vs non-exact\n",
    "        if state['exact'].empty or state['non_exact'].empty:\n",
    "            timestamped_print(\"Splitting exact vs non-exact…\")\n",
    "            df = state['combined'].copy()\n",
    "\n",
    "            # ensure HPO_Term column is present\n",
    "            if 'HPO_Term' not in df.columns:\n",
    "                df['HPO_Term'] = np.nan\n",
    "\n",
    "            # compute HPO_Term for any missing\n",
    "            if not df.empty:\n",
    "                df['HPO_Term'] = (\n",
    "                    df.apply(\n",
    "                        lambda r: extract_hpo_term(r['phrase'], r['unique_metadata'], cluster_index)\n",
    "                                if pd.isna(r['HPO_Term']) else r['HPO_Term'],\n",
    "                        axis=1\n",
    "                    )\n",
    "                    .astype(object)\n",
    "                    .where(lambda x: pd.notna(x), np.nan)\n",
    "                )\n",
    "\n",
    "            exact_df, non_exact_df = split_exact_nonexact(df, hpo_term_col='HPO_Term')\n",
    "            state['exact']     = exact_df\n",
    "            state['non_exact'] = non_exact_df\n",
    "            save_state_checkpoint(state, TEMP_FILES, keys=['exact', 'non_exact'])\n",
    "        else:\n",
    "            timestamped_print(\"Skipped splitting; checkpoints exist.\")\n",
    "\n",
    "        # 7) post-process non-exact entries\n",
    "        non_ex = state['non_exact'].copy()\n",
    "        for col in (\"llm_parse_reason\", \"raw_llm_resp\"):\n",
    "            non_ex[col] = non_ex.get(col, pd.Series(dtype=\"object\")).astype(\"object\")\n",
    "\n",
    "        idxs = non_ex[\n",
    "            (non_ex['category'] == 'Abnormal') &\n",
    "            (non_ex['HPO_Term'].isna()) &\n",
    "            (non_ex['HPO_Term'] != \"No Candidate Fit\")\n",
    "        ].index\n",
    "\n",
    "        if not idxs.empty:\n",
    "            timestamped_print(f\"Generating HPO for {len(idxs)} entries...\")\n",
    "            counter = 0\n",
    "            for i, idx in enumerate(tqdm(idxs, desc=\"Generating HPO\", unit=\"entry\")):\n",
    "                row_df = non_ex.loc[[idx]]\n",
    "                out_df = generate_hpo_terms(row_df, system_message_II)\n",
    "                hp     = (out_df.at[0, 'HPO_Terms'][0]['HPO_Term']\n",
    "                          if not out_df.empty else None)\n",
    "                # attach parse reason & raw response\n",
    "                if 'llm_parse_reason' in out_df.columns:\n",
    "                    non_ex.at[idx, 'llm_parse_reason'] = out_df.at[0, 'llm_parse_reason']\n",
    "                if 'raw_llm_resp' in out_df.columns:\n",
    "                    non_ex.at[idx, 'raw_llm_resp']       = out_df.at[0, 'raw_llm_resp']\n",
    "                non_ex.at[idx, 'HPO_Term'] = hp or \"No Candidate Fit\"\n",
    "\n",
    "                counter += 1\n",
    "                if counter >= 50 or (i + 1) == len(idxs):\n",
    "                    state['non_exact'] = non_ex.copy()\n",
    "                    save_state_checkpoint(state, TEMP_FILES, keys=['non_exact'])\n",
    "                    counter = 0\n",
    "        else:\n",
    "            timestamped_print(\"No non-exact entries to process.\")\n",
    "\n",
    "        # 8) compile final results\n",
    "        if state['final'].empty:\n",
    "            timestamped_print(\"Compiling final results...\")\n",
    "            merged = pd.concat([state['exact'], state['non_exact']], ignore_index=True)\n",
    "            merged = merged.dropna(subset=['HPO_Term'])\n",
    "            if not merged.empty:\n",
    "                grouped = (\n",
    "                    merged\n",
    "                      .groupby('patient_id')[['phrase','category','HPO_Term']]\n",
    "                      .apply(lambda g: g.to_dict('records'))\n",
    "                      .reset_index(name='HPO_Terms')\n",
    "                )\n",
    "                state['final'] = grouped.copy()\n",
    "            else:\n",
    "                state['final'] = pd.DataFrame(columns=['patient_id','HPO_Terms'])\n",
    "            save_state_checkpoint(state, TEMP_FILES, keys=['final'])\n",
    "        else:\n",
    "            timestamped_print(\"Skipped final compilation; checkpoint exists.\")\n",
    "\n",
    "        # 9) output\n",
    "        process_results(state['final'])\n",
    "        success = True\n",
    "        logger.log(f\"Pipeline completed in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.log(f\"Pipeline error: {e}. Saving progress and exiting.\")\n",
    "        save_state_checkpoint(state, TEMP_FILES)\n",
    "        traceback.print_exc()\n",
    "        # sys.exit(1)\n",
    "\n",
    "    finally:\n",
    "        cleanup(TEMP_FILES, success)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1768cb39",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 86) (4017429976.py, line 86)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31minput_file = \"/Users/garciabt\u001b[39m\n                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 86)\n"
     ]
    }
   ],
   "source": [
    "# ======================= Batch HPO Term Replacement =======================\n",
    "import csv\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def get_ols_term_status(hpo_id, max_retries=3, sleep_seconds=1):\n",
    "    # Checks OLS for HPO term status and replacement\n",
    "    if not hpo_id.startswith(\"HP:\"):\n",
    "        print(f\"[SKIP] Invalid HPO ID format: {hpo_id}\")\n",
    "        return False, None, \"invalid_format\"\n",
    "    iri = f\"http://purl.obolibrary.org/obo/{hpo_id.replace(':', '_')}\"\n",
    "    url = f\"https://www.ebi.ac.uk/ols4/api/ontologies/hp/terms?iri={requests.utils.quote(iri)}\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            term = data.get(\"_embedded\", {}).get(\"terms\", [{}])[0]\n",
    "            is_obsolete = term.get(\"is_obsolete\", False)\n",
    "            replacement_id = term.get(\"term_replaced_by\")\n",
    "            return is_obsolete, replacement_id, None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[WARN] {hpo_id} failed (attempt {attempt}): {e}\")\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(sleep_seconds * attempt)\n",
    "            else:\n",
    "                return False, None, str(e)\n",
    "    return False, None, \"unknown_error\"\n",
    "\n",
    "def process_and_replace_all(infile, outfile):\n",
    "    # Reads CSV, replaces obsolete HPO terms, writes updated CSV\n",
    "    failures = []\n",
    "    replaced_count = 0\n",
    "    skipped_count = 0\n",
    "    with open(infile, newline='') as rf:\n",
    "        reader = list(csv.DictReader(rf))\n",
    "        fieldnames = reader[0].keys() | {\"was_replaced\", \"original_term\"}\n",
    "        with open(outfile, 'w', newline='') as wf:\n",
    "            writer = csv.DictWriter(wf, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for row in tqdm(reader, desc=\"Processing HPO terms\", unit=\"row\"):\n",
    "                hpo_field = row.get(\"hpo_term\", \"\").strip()\n",
    "                if not hpo_field or hpo_field.lower() == \"none\":\n",
    "                    row[\"original_term\"] = hpo_field\n",
    "                    row[\"was_replaced\"] = \"False\"\n",
    "                    skipped_count += 1\n",
    "                    writer.writerow(row)\n",
    "                    continue\n",
    "                hpo_ids = [h.strip() for h in hpo_field.split(\",\") if h.strip().startswith(\"HP:\")]\n",
    "                replaced_ids = []\n",
    "                was_any_replaced = False\n",
    "                for hpo_id in hpo_ids:\n",
    "                    is_obs, replacement, error = get_ols_term_status(hpo_id)\n",
    "                    if error:\n",
    "                        failures.append({\"hpo_id\": hpo_id, \"error\": error, \"row\": row})\n",
    "                        replaced_ids.append(hpo_id)\n",
    "                        continue\n",
    "                    if is_obs and replacement:\n",
    "                        replaced_ids.append(replacement)\n",
    "                        was_any_replaced = True\n",
    "                        replaced_count += 1\n",
    "                        print(f\"[REPLACED] {hpo_id} → {replacement}\")\n",
    "                    elif is_obs and not replacement:\n",
    "                        print(f\"[OBSOLETE w/o replacement] {hpo_id}\")\n",
    "                        replaced_ids.append(hpo_id)\n",
    "                    else:\n",
    "                        replaced_ids.append(hpo_id)\n",
    "                row[\"original_term\"] = hpo_field\n",
    "                row[\"hpo_term\"] = \", \".join(replaced_ids)\n",
    "                row[\"was_replaced\"] = str(was_any_replaced)\n",
    "                writer.writerow(row)\n",
    "    print(f\"\\n✅ Batch complete. Updated file saved to:\\n{outfile}\")\n",
    "    print(f\"🟢 Terms replaced: {replaced_count}\")\n",
    "    print(f\"🟡 Skipped (empty or 'none'): {skipped_count}\")\n",
    "    print(f\"🔴 Failed API lookups: {len(failures)}\")\n",
    "    if failures:\n",
    "        failfile = outfile.replace(\".csv\", \"_failures.csv\")\n",
    "        with open(failfile, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\"hpo_id\", \"error\", \"row\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(failures)\n",
    "        print(f\"🗂️ Failures saved to: {failfile}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"/Users/garciabt/Library/CloudStorage/OneDrive-BaylorCollegeofMedicine/01 Posey Lab/RAG-HPO/RAG-HPO/Test_Cases_Manual_Annotations.csv\"\n",
    "    output_file = \"/Users/garciabt/Library/CloudStorage/OneDrive-BaylorCollegeofMedicine/01 Posey Lab/RAG-HPO/RAG-HPO/Test_Cases_Manual_Annotations_updated.csv\"\n",
    "    process_and_replace_all(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359f751d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Replaced 11 alt_id entries with canonical hp_id\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re  # [Added] for splitting alt_ids\n",
    "\n",
    "def enrich_results_with_hpo_info(results_csv: str,\n",
    "                                 hpo_full_csv: str,\n",
    "                                 output_csv: str):\n",
    "    \"\"\"\n",
    "    Read your pipeline results (with column 'hpo_term') and the full HPO terms file,\n",
    "    replace any alt_id in 'hpo_term' with its canonical hp_id, then merge in\n",
    "    alt_ids, snomedct, and umls by matching hpo_term → hp_id, and save.\n",
    "    \"\"\"\n",
    "    # 1. Load the results file, treating all columns as strings\n",
    "    results_df = pd.read_csv(results_csv, dtype=str).fillna('')\n",
    "\n",
    "    # 2. Load the full HPO terms file and extract unique info rows\n",
    "    hpo_full_df = pd.read_csv(hpo_full_csv, dtype=str).fillna('')\n",
    "    hpo_info_df = (\n",
    "        hpo_full_df\n",
    "        .loc[:, ['hp_id', 'alt_ids', 'snomedct', 'umls']]\n",
    "        .drop_duplicates(subset=['hp_id'])\n",
    "    )\n",
    "\n",
    "    # 3. Build alt_id → canonical hp_id mapping                 # [Added]\n",
    "    alt_map = {}\n",
    "    for hp_id, alt_ids in zip(hpo_info_df['hp_id'], hpo_info_df['alt_ids']):\n",
    "        if not alt_ids:\n",
    "            continue\n",
    "        for alt in re.split(r'[;,]\\s*', alt_ids):\n",
    "            alt = alt.strip()\n",
    "            if alt:\n",
    "                alt_map[alt] = hp_id\n",
    "\n",
    "    # 4. Replace alt_ids in results_df['hpo_term']                # [Modified]\n",
    "    original_terms = results_df['hpo_term'].copy()               # [Added]\n",
    "    results_df['hpo_term'] = results_df['hpo_term'].apply(\n",
    "        lambda x: alt_map.get(x, x)\n",
    "    )\n",
    "    num_replaced = (results_df['hpo_term'] != original_terms).sum()  # [Added]\n",
    "    print(f\"[INFO] Replaced {num_replaced} alt_id entries with canonical hp_id\")  # [Added]\n",
    "\n",
    "    # 5. Merge in HPO info\n",
    "    enriched_df = results_df.merge(\n",
    "        hpo_info_df,\n",
    "        how='left',\n",
    "        left_on='hpo_term',\n",
    "        right_on='hp_id'\n",
    "    )\n",
    "\n",
    "    # 6. Drop redundant hp_id column\n",
    "    enriched_df.drop(columns=['hp_id'], inplace=True)\n",
    "\n",
    "    # 7. Write out enriched DataFrame\n",
    "    enriched_df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Example invocation:\n",
    "enrich_results_with_hpo_info(\n",
    "    results_csv='Test_Cases_Manual_Annotations.csv',\n",
    "    hpo_full_csv='hpo_terms_full.csv',\n",
    "    output_csv='CSC_Annotations_enriched.csv'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
